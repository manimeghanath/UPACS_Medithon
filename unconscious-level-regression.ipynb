{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53518039",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-02T21:15:04.324314Z",
     "iopub.status.busy": "2025-09-02T21:15:04.324029Z",
     "iopub.status.idle": "2025-09-02T21:15:06.131233Z",
     "shell.execute_reply": "2025-09-02T21:15:06.130343Z"
    },
    "papermill": {
     "duration": 1.817688,
     "end_time": "2025-09-02T21:15:06.132894",
     "exception": false,
     "start_time": "2025-09-02T21:15:04.315206",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/physionet-ecg/scg_rhc_f16.parquet\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b51e1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T10:45:38.480848Z",
     "iopub.status.busy": "2025-09-02T10:45:38.480529Z",
     "iopub.status.idle": "2025-09-02T10:45:45.176278Z",
     "shell.execute_reply": "2025-09-02T10:45:45.175013Z",
     "shell.execute_reply.started": "2025-09-02T10:45:38.480813Z"
    },
    "papermill": {
     "duration": 0.005735,
     "end_time": "2025-09-02T21:15:06.144959",
     "exception": false,
     "start_time": "2025-09-02T21:15:06.139224",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Run this cell first. It installs a few libs not always preinstalled.\n",
    "!pip install --quiet neurokit2 xgboost pyarrow tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1201fb8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T10:50:57.216564Z",
     "iopub.status.busy": "2025-09-02T10:50:57.216176Z",
     "iopub.status.idle": "2025-09-02T10:50:58.054995Z",
     "shell.execute_reply": "2025-09-02T10:50:58.053671Z",
     "shell.execute_reply.started": "2025-09-02T10:50:57.216538Z"
    },
    "papermill": {
     "duration": 0.005572,
     "end_time": "2025-09-02T21:15:06.156318",
     "exception": false,
     "start_time": "2025-09-02T21:15:06.150746",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the parquet file\n",
    "file_path = \"/kaggle/input/physionet-ecg/scg_rhc_f16.parquet\"\n",
    "\n",
    "# Load the parquet file\n",
    "df = pd.read_parquet(file_path)\n",
    "\n",
    "# Show dataset shape and first rows\n",
    "print(\"Shape of dataset:\", df.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e644a23",
   "metadata": {
    "papermill": {
     "duration": 0.005497,
     "end_time": "2025-09-02T21:15:06.167605",
     "exception": false,
     "start_time": "2025-09-02T21:15:06.162108",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03c3f7ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T21:15:06.180996Z",
     "iopub.status.busy": "2025-09-02T21:15:06.180219Z",
     "iopub.status.idle": "2025-09-02T21:15:11.736524Z",
     "shell.execute_reply": "2025-09-02T21:15:11.735446Z"
    },
    "papermill": {
     "duration": 5.564923,
     "end_time": "2025-09-02T21:15:11.738339",
     "exception": false,
     "start_time": "2025-09-02T21:15:06.173416",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m708.4/708.4 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install --quiet neurokit2 xgboost pyarrow tqdm matplotlib seaborn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b28176",
   "metadata": {
    "papermill": {
     "duration": 0.005772,
     "end_time": "2025-09-02T21:15:11.750851",
     "exception": false,
     "start_time": "2025-09-02T21:15:11.745079",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load parquet and inspect ECG lead II contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acca697",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T11:53:44.616873Z",
     "iopub.status.busy": "2025-09-02T11:53:44.616533Z",
     "iopub.status.idle": "2025-09-02T11:53:45.516115Z",
     "shell.execute_reply": "2025-09-02T11:53:45.514577Z",
     "shell.execute_reply.started": "2025-09-02T11:53:44.616846Z"
    },
    "papermill": {
     "duration": 0.005536,
     "end_time": "2025-09-02T21:15:11.762268",
     "exception": false,
     "start_time": "2025-09-02T21:15:11.756732",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt, seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "pd.set_option('display.max_columns', 200)\n",
    "\n",
    "pq_path = \"/kaggle/input/physionet-ecg/scg_rhc_f16.parquet\"   # your file\n",
    "df = pd.read_parquet(pq_path)\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "df.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbcf57f",
   "metadata": {
    "papermill": {
     "duration": 0.006387,
     "end_time": "2025-09-02T21:15:11.774450",
     "exception": false,
     "start_time": "2025-09-02T21:15:11.768063",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Quick diagnostic of ECG_lead_II arrays: lengths, sample stats, plot examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dfa740",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T11:54:52.956577Z",
     "iopub.status.busy": "2025-09-02T11:54:52.956173Z",
     "iopub.status.idle": "2025-09-02T11:54:53.793074Z",
     "shell.execute_reply": "2025-09-02T11:54:53.791852Z",
     "shell.execute_reply.started": "2025-09-02T11:54:52.956553Z"
    },
    "papermill": {
     "duration": 0.005536,
     "end_time": "2025-09-02T21:15:11.785790",
     "exception": false,
     "start_time": "2025-09-02T21:15:11.780254",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# examine the type & lengths of ECG_lead_II content\n",
    "ecg_col = 'ECG_lead_II'\n",
    "assert ecg_col in df.columns, f\"{ecg_col} not present\"\n",
    "\n",
    "lengths = []\n",
    "mins = []\n",
    "maxs = []\n",
    "for i, arr in enumerate(df[ecg_col]):\n",
    "    # arr is likely a list or numpy array; coerce to numpy\n",
    "    a = np.asarray(arr, dtype=float)\n",
    "    lengths.append(a.size)\n",
    "    mins.append(a.min())\n",
    "    maxs.append(a.max())\n",
    "    \n",
    "lengths = np.array(lengths)\n",
    "print(\"ECG_lead_II lengths: min, median, max =\", lengths.min(), np.median(lengths), lengths.max())\n",
    "print(\"ECG amplitude: min mean, max mean =\", np.mean(mins), np.mean(maxs))\n",
    "\n",
    "# show distribution of lengths\n",
    "plt.figure(figsize=(6,3))\n",
    "sns.histplot(lengths, bins=30)\n",
    "plt.title(\"Distribution of ECG_lead_II sample counts per row\")\n",
    "plt.xlabel(\"Number of samples\")\n",
    "plt.show()\n",
    "\n",
    "# plot first 3 ECG traces (normalized)\n",
    "plt.figure(figsize=(12,6))\n",
    "for i in range(3):\n",
    "    a = np.asarray(df[ecg_col].iloc[i], dtype=float)\n",
    "    t = np.arange(a.size)\n",
    "    # normalize for visualization\n",
    "    a = (a - a.mean()) / (a.std() + 1e-9)\n",
    "    plt.plot(t, a + i*6, label=f\"rec{i}\")  # offset vertically\n",
    "plt.title(\"First 3 ECG_lead_II traces (standardized, vertically offset)\")\n",
    "plt.xlabel(\"Sample index\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bedfc2",
   "metadata": {
    "papermill": {
     "duration": 0.005489,
     "end_time": "2025-09-02T21:15:11.797387",
     "exception": false,
     "start_time": "2025-09-02T21:15:11.791898",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Auto-select plausible sampling rate per recording"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9bcae2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T11:56:50.840160Z",
     "iopub.status.busy": "2025-09-02T11:56:50.839016Z",
     "iopub.status.idle": "2025-09-02T11:56:52.468366Z",
     "shell.execute_reply": "2025-09-02T11:56:52.467055Z",
     "shell.execute_reply.started": "2025-09-02T11:56:50.840130Z"
    },
    "papermill": {
     "duration": 0.005503,
     "end_time": "2025-09-02T21:15:11.808956",
     "exception": false,
     "start_time": "2025-09-02T21:15:11.803453",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "import neurokit2 as nk\n",
    "\n",
    "candidate_fs = [125, 250, 500]\n",
    "def estimate_fs_for_signal(arr):\n",
    "    arr = np.asarray(arr, dtype=float)\n",
    "    best_fs = None\n",
    "    best_hr_diff = 1e9\n",
    "    best_hr = None\n",
    "    for fs in candidate_fs:\n",
    "        try:\n",
    "            # use ECG peak detector only (fast)\n",
    "            _, rpeaks = nk.ecg_peaks(arr, sampling_rate=fs)\n",
    "            r_inds = rpeaks.get('ECG_R_Peaks', [])\n",
    "            if len(r_inds) < 3:\n",
    "                continue\n",
    "            rr = np.diff(np.array(r_inds)) / fs\n",
    "            mean_hr = 60.0 / rr.mean()\n",
    "            # prefer fs where HR falls in plausible range\n",
    "            diff = 0 if (40 <= mean_hr <= 140) else abs(mean_hr - 80)\n",
    "            if diff < best_hr_diff:\n",
    "                best_hr_diff = diff\n",
    "                best_fs = fs\n",
    "                best_hr = mean_hr\n",
    "        except Exception:\n",
    "            continue\n",
    "    return best_fs, best_hr\n",
    "\n",
    "# test on first 30 recordings\n",
    "estimates = []\n",
    "for i in range(min(30, len(df))):\n",
    "    fs, hr = estimate_fs_for_signal(df[ecg_col].iloc[i])\n",
    "    estimates.append((i, fs, hr))\n",
    "\n",
    "pd.DataFrame(estimates, columns=['idx','chosen_fs','mean_hr']).head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31933581",
   "metadata": {
    "papermill": {
     "duration": 0.005532,
     "end_time": "2025-09-02T21:15:11.820262",
     "exception": false,
     "start_time": "2025-09-02T21:15:11.814730",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Windowing + HRV feature extraction functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45980f71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T12:00:45.012822Z",
     "iopub.status.busy": "2025-09-02T12:00:45.012170Z",
     "iopub.status.idle": "2025-09-02T12:00:45.026192Z",
     "shell.execute_reply": "2025-09-02T12:00:45.025033Z",
     "shell.execute_reply.started": "2025-09-02T12:00:45.012796Z"
    },
    "papermill": {
     "duration": 0.005647,
     "end_time": "2025-09-02T21:15:11.831668",
     "exception": false,
     "start_time": "2025-09-02T21:15:11.826021",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "from scipy.signal import butter, filtfilt, iirnotch\n",
    "\n",
    "def bandpass(sig, fs, low=0.5, high=40, order=4):\n",
    "    from scipy.signal import butter, filtfilt\n",
    "    b,a = butter(order, [low/(fs/2), high/(fs/2)], btype='band')\n",
    "    return filtfilt(b,a,sig)\n",
    "\n",
    "def notch(sig, fs, freq=50.0, q=30.0):\n",
    "    from scipy.signal import iirnotch, filtfilt\n",
    "    b,a = iirnotch(freq/(fs/2), q)\n",
    "    return filtfilt(b,a,sig)\n",
    "\n",
    "def extract_hrv_features_from_window(w, fs):\n",
    "    \"\"\"Return dict of HRV/time-domain features for window w (1D numpy array).\"\"\"\n",
    "    out = {}\n",
    "    # basic amplitude check\n",
    "    out['max_abs'] = float(np.max(np.abs(w)))\n",
    "    if out['max_abs'] < 1e-6:\n",
    "        out['sqi'] = 0\n",
    "        return out\n",
    "    # filter (safe try)\n",
    "    try:\n",
    "        wf = bandpass(w, fs)\n",
    "        wf = notch(wf, fs, freq=50.0)\n",
    "    except Exception:\n",
    "        wf = w.copy()\n",
    "    # detect R peaks robustly\n",
    "    try:\n",
    "        signals, info = nk.ecg_process(wf, sampling_rate=fs)\n",
    "        rpeaks = info.get(\"ECG_R_Peaks\", [])\n",
    "        rpeaks = np.array(rpeaks)\n",
    "    except Exception:\n",
    "        rpeaks = np.array(nk.ecg_peaks(wf, sampling_rate=fs)[1].get('ECG_R_Peaks', []))\n",
    "    if len(rpeaks) < 4:\n",
    "        out['sqi'] = 0\n",
    "        return out\n",
    "    rr_s = np.diff(rpeaks) / fs  # in seconds\n",
    "    rr_ms = rr_s * 1000.0\n",
    "    out['sqi'] = 1\n",
    "    out['mean_hr'] = float(60.0 / np.mean(rr_s))\n",
    "    out['sdnn'] = float(np.std(rr_ms, ddof=1))\n",
    "    # rmssd\n",
    "    if len(rr_ms) >= 3:\n",
    "        diff_rr = np.diff(rr_ms)\n",
    "        out['rmssd'] = float(np.sqrt(np.mean(diff_rr**2)))\n",
    "        out['pnn50'] = float(np.mean(np.abs(diff_rr) > 50.0))\n",
    "    else:\n",
    "        out['rmssd'] = np.nan\n",
    "        out['pnn50'] = np.nan\n",
    "    # sample entropy (RR in ms)\n",
    "    try:\n",
    "        out['sampen'] = float(nk.entropy_sample(rr_ms, normalize=True))\n",
    "    except Exception:\n",
    "        out['sampen'] = np.nan\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2b2ae9",
   "metadata": {
    "papermill": {
     "duration": 0.005717,
     "end_time": "2025-09-02T21:15:11.843204",
     "exception": false,
     "start_time": "2025-09-02T21:15:11.837487",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Build feature windows across recordings and save to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba4cfb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T12:01:15.150910Z",
     "iopub.status.busy": "2025-09-02T12:01:15.150600Z",
     "iopub.status.idle": "2025-09-02T12:01:44.477597Z",
     "shell.execute_reply": "2025-09-02T12:01:44.476584Z",
     "shell.execute_reply.started": "2025-09-02T12:01:15.150890Z"
    },
    "papermill": {
     "duration": 0.005546,
     "end_time": "2025-09-02T21:15:11.854606",
     "exception": false,
     "start_time": "2025-09-02T21:15:11.849060",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "import math, os\n",
    "features = []\n",
    "max_recs = 200   # limit for speed; increase if you want more data\n",
    "win_s = 30\n",
    "hop_s = 15\n",
    "\n",
    "for idx, row in tqdm(df.iloc[:max_recs].iterrows(), total=min(max_recs, len(df))):\n",
    "    arr = np.asarray(row[ecg_col], dtype=float)\n",
    "    fs, hr = estimate_fs_for_signal(arr)\n",
    "    if fs is None:\n",
    "        # if we couldn't detect fs, skip this recording\n",
    "        continue\n",
    "    npts = arr.size\n",
    "    nwin = (npts - int(win_s*fs)) // int(hop_s*fs) + 1\n",
    "    if nwin <= 0:\n",
    "        # if recording too short for a single window, skip\n",
    "        continue\n",
    "    for start in range(0, npts - int(win_s*fs) + 1, int(hop_s*fs)):\n",
    "        w = arr[start:start+int(win_s*fs)]\n",
    "        feats = extract_hrv_features_from_window(w, fs)\n",
    "        if feats.get('sqi', 0) == 0:\n",
    "            continue\n",
    "        rec = {\n",
    "            'rec_index': int(idx),\n",
    "            'start_sample': int(start),\n",
    "            'fs': int(fs),\n",
    "            'win_s': win_s,\n",
    "            'n_samples': int(w.size),\n",
    "        }\n",
    "        rec.update(feats)\n",
    "        # optional metadata\n",
    "        if 'record_name' in df.columns:\n",
    "            rec['record_name'] = row['record_name']\n",
    "        features.append(rec)\n",
    "\n",
    "feats_df = pd.DataFrame(features)\n",
    "print(\"Feature windows extracted:\", feats_df.shape)\n",
    "feats_df.head()\n",
    "feats_df.to_csv('ecg_feature_windows_scg_rhc_sample.csv', index=False)\n",
    "print(\"Saved ecg_feature_windows_scg_rhc_sample.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8ea576",
   "metadata": {
    "papermill": {
     "duration": 0.005528,
     "end_time": "2025-09-02T21:15:11.865855",
     "exception": false,
     "start_time": "2025-09-02T21:15:11.860327",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Quick unsupervised baseline: KMeans clustering + visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8856815",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T12:16:03.561533Z",
     "iopub.status.busy": "2025-09-02T12:16:03.560695Z",
     "iopub.status.idle": "2025-09-02T12:16:04.146254Z",
     "shell.execute_reply": "2025-09-02T12:16:04.145100Z",
     "shell.execute_reply.started": "2025-09-02T12:16:03.561503Z"
    },
    "papermill": {
     "duration": 0.00544,
     "end_time": "2025-09-02T21:15:11.877138",
     "exception": false,
     "start_time": "2025-09-02T21:15:11.871698",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# dynamically pick only available numeric HRV columns\n",
    "candidate_cols = ['mean_hr','sdnn','rmssd','pnn50','sampen']\n",
    "num_cols = [c for c in candidate_cols if c in feats_df.columns and feats_df[c].notna().sum() > 0]\n",
    "print(\"Using feature columns:\", num_cols)\n",
    "\n",
    "# filter rows where at least these columns are non-null\n",
    "feats_df_clean = feats_df.dropna(subset=num_cols).copy()\n",
    "print(\"Remaining rows after dropna:\", len(feats_df_clean))\n",
    "\n",
    "# prepare features\n",
    "X = feats_df_clean[num_cols].values\n",
    "scaler = StandardScaler()\n",
    "Xs = scaler.fit_transform(X)\n",
    "\n",
    "# PCA for visualization\n",
    "pca = PCA(n_components=2)\n",
    "Xp = pca.fit_transform(Xs)\n",
    "\n",
    "# KMeans clustering\n",
    "k = 3\n",
    "km = KMeans(n_clusters=k, random_state=42, n_init=20)\n",
    "labels = km.fit_predict(Xs)\n",
    "sil = silhouette_score(Xs, labels)\n",
    "print(\"Silhouette score (k=3):\", sil)\n",
    "\n",
    "feats_df_clean['cluster'] = labels\n",
    "\n",
    "# visualize clusters\n",
    "plt.figure(figsize=(7,5))\n",
    "sns.scatterplot(x=Xp[:,0], y=Xp[:,1], hue=labels, palette='tab10', s=40, alpha=0.8)\n",
    "plt.title(\"PCA of HRV features colored by KMeans cluster\")\n",
    "plt.show()\n",
    "\n",
    "# cluster centers in original feature space\n",
    "centers = scaler.inverse_transform(km.cluster_centers_)\n",
    "centers_df = pd.DataFrame(centers, columns=num_cols)\n",
    "print(\"Cluster centers (HRV features):\")\n",
    "display(centers_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b237e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T12:20:04.372102Z",
     "iopub.status.busy": "2025-09-02T12:20:04.371743Z",
     "iopub.status.idle": "2025-09-02T12:20:05.082902Z",
     "shell.execute_reply": "2025-09-02T12:20:05.081892Z",
     "shell.execute_reply.started": "2025-09-02T12:20:04.372077Z"
    },
    "papermill": {
     "duration": 0.005568,
     "end_time": "2025-09-02T21:15:11.888506",
     "exception": false,
     "start_time": "2025-09-02T21:15:11.882938",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# assign ordering: higher sdnn -> lighter state; so depth_score = descending sdnn ranking\n",
    "centers_df['cluster'] = centers_df.index\n",
    "order = centers_df.sort_values('sdnn', ascending=False)['cluster'].tolist()\n",
    "# map cluster -> pseudo depth (0 = light, 2 = deep) - invert if you prefer\n",
    "cluster_to_depth = {c: i for i,c in enumerate(order)}  \n",
    "print(\"cluster_to_depth mapping:\", cluster_to_depth)\n",
    "\n",
    "feats_df_clean['pseudo_depth'] = feats_df_clean['cluster'].map(cluster_to_depth)\n",
    "\n",
    "# train/test split by rec_index (to avoid leakage across windows)\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "groups = feats_df_clean['rec_index'].values\n",
    "X = feats_df_clean[num_cols].fillna(0.0).values\n",
    "y = feats_df_clean['pseudo_depth'].values\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, test_idx = next(gss.split(X, y, groups))\n",
    "X_train, X_test = X[train_idx], X[test_idx]\n",
    "y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "import xgboost as xgb\n",
    "model = xgb.XGBRegressor(n_estimators=100, max_depth=4, learning_rate=0.05, random_state=42)\n",
    "model.fit(X_train, y_train, eval_set=[(X_test,y_test)], early_stopping_rounds=10, verbose=False)\n",
    "\n",
    "preds = model.predict(X_test)\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "print(\"MAE:\", mean_absolute_error(y_test, preds))\n",
    "print(\"R2:\", r2_score(y_test, preds))\n",
    "\n",
    "# Save model to file\n",
    "import joblib\n",
    "joblib.dump(model, 'xgb_pseudo_depth_model.joblib')\n",
    "print(\"Saved xgb_pseudo_depth_model.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dc5060",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T12:49:38.602441Z",
     "iopub.status.busy": "2025-09-02T12:49:38.602114Z",
     "iopub.status.idle": "2025-09-02T12:49:38.628565Z",
     "shell.execute_reply": "2025-09-02T12:49:38.626145Z",
     "shell.execute_reply.started": "2025-09-02T12:49:38.602419Z"
    },
    "papermill": {
     "duration": 0.0058,
     "end_time": "2025-09-02T21:15:11.900209",
     "exception": false,
     "start_time": "2025-09-02T21:15:11.894409",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "'''import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# use the detected column, or fail with clear message\n",
    "if ecg_col_found is None:\n",
    "    raise RuntimeError(\"No ECG column detected in df. Re-run the parquet load cell or check the file.\")\n",
    "\n",
    "# ensure rec_index values are valid indices (we used iloc, so rec_idx must be < len(df))\n",
    "valid_recs = feats_df_clean['rec_index'].dropna().astype(int).unique()\n",
    "print(\"Unique rec_index count in features:\", len(valid_recs), \" (example first 10):\", valid_recs[:10])\n",
    "\n",
    "for c in sorted(feats_df_clean['cluster'].unique()):\n",
    "    # choose first window row for this cluster\n",
    "    row = feats_df_clean[feats_df_clean['cluster']==c].iloc[0]\n",
    "    rec_idx = int(row['rec_index'])\n",
    "    start = int(row['start_sample'])\n",
    "    fs = int(row['fs'])\n",
    "    n_samples = int(row['n_samples'])\n",
    "\n",
    "    # defensive checks\n",
    "    if rec_idx < 0 or rec_idx >= len(df):\n",
    "        print(f\"Skipping cluster {c}: rec_idx {rec_idx} out of range (0..{len(df)-1})\")\n",
    "        continue\n",
    "\n",
    "    # pull the array safely using iloc and the detected column name\n",
    "    try:\n",
    "        arr_raw = df.iloc[rec_idx][ecg_col_found]\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping cluster {c}: cannot access ECG at rec {rec_idx}: {e}\")\n",
    "        continue\n",
    "\n",
    "    arr = np.asarray(arr_raw, dtype=float)\n",
    "    if arr.size == 0:\n",
    "        print(f\"Skipping cluster {c}: ECG array for rec {rec_idx} is empty\")\n",
    "        continue\n",
    "\n",
    "    # clip end index safely\n",
    "    end = start + n_samples\n",
    "    if start >= arr.size:\n",
    "        print(f\"Cluster {c} rec {rec_idx}: start {start} >= arr length {arr.size} -> skipping\")\n",
    "        continue\n",
    "    if end > arr.size:\n",
    "        end = arr.size\n",
    "\n",
    "    w = arr[start:end]\n",
    "    if w.size == 0:\n",
    "        print(f\"Cluster {c} rec {rec_idx}: extracted window is empty -> skipping\")\n",
    "        continue\n",
    "\n",
    "    # time axis: if n_samples was for standardized window, scale time proportionally\n",
    "    win_duration = row.get('win_s', n_samples / fs if fs>0 else 1.0)\n",
    "    t = np.linspace(0, win_duration * (w.size / max(n_samples,1)), w.size)\n",
    "\n",
    "    plt.figure(figsize=(9,2.2))\n",
    "    plt.plot(t, (w - w.mean()) / (w.std()+1e-9))\n",
    "    plt.title(f\"Cluster {c} example window (rec {rec_idx}, start {start}, end {end}, fs {fs})\")\n",
    "    plt.xlabel(\"seconds\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97a3d19",
   "metadata": {
    "papermill": {
     "duration": 0.005752,
     "end_time": "2025-09-02T21:15:11.911915",
     "exception": false,
     "start_time": "2025-09-02T21:15:11.906163",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# FULLL CODDEE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89de9067",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T15:39:30.501855Z",
     "iopub.status.busy": "2025-09-02T15:39:30.501500Z",
     "iopub.status.idle": "2025-09-02T15:40:11.406750Z",
     "shell.execute_reply": "2025-09-02T15:40:11.405406Z",
     "shell.execute_reply.started": "2025-09-02T15:39:30.501826Z"
    },
    "papermill": {
     "duration": 0.005764,
     "end_time": "2025-09-02T21:15:11.923595",
     "exception": false,
     "start_time": "2025-09-02T21:15:11.917831",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Full end-to-end Kaggle-ready notebook\n",
    "# Copy this whole block into a Kaggle Python notebook and run‚Äîmake sure the dataset path matches.\n",
    "\n",
    "# ========================================================\n",
    "# 0) Installs (run once at top)\n",
    "# ========================================================\n",
    "!pip install --quiet neurokit2 xgboost joblib seaborn tqdm\n",
    "\n",
    "# ========================================================\n",
    "# 1) Imports & constants\n",
    "# ========================================================\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (9,4)\n",
    "\n",
    "# Modeling imports\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "\n",
    "# Signal processing\n",
    "import neurokit2 as nk\n",
    "from scipy.signal import butter, filtfilt, iirnotch\n",
    "\n",
    "# ========================================================\n",
    "# 2) Load parquet and inspect\n",
    "# ========================================================\n",
    "PQ_PATH = \"/kaggle/input/physionet-ecg/scg_rhc_f16.parquet\"  # update if necessary\n",
    "assert os.path.exists(PQ_PATH), f\"Parquet file not found at {PQ_PATH}\"\n",
    "df = pd.read_parquet(PQ_PATH)\n",
    "print(\"Loaded parquet:\", PQ_PATH)\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "\n",
    "# find a good ECG column (prefer ECG_lead_II if present)\n",
    "preferred = [\"ECG_lead_II\", \"ECG_lead_ii\", \"ECG_lead_II \", \"ECG_lead_I\", \"ecg_lead_ii\"]\n",
    "ecg_col = None\n",
    "for p in preferred:\n",
    "    if p in df.columns:\n",
    "        ecg_col = p\n",
    "        break\n",
    "# fallback: first column containing 'ecg'\n",
    "if ecg_col is None:\n",
    "    for c in df.columns:\n",
    "        if 'ecg' in c.lower():\n",
    "            ecg_col = c\n",
    "            break\n",
    "\n",
    "if ecg_col is None:\n",
    "    raise RuntimeError(\"No ECG column found in the parquet file. Inspect df.columns.\")\n",
    "print(\"Using ECG column:\", ecg_col)\n",
    "\n",
    "# ========================================================\n",
    "# 3) Diagnostics: lengths, amplitude, plot a few traces\n",
    "# ========================================================\n",
    "lengths = []\n",
    "mins = []\n",
    "maxs = []\n",
    "for arr in df[ecg_col]:\n",
    "    a = np.asarray(arr, dtype=float)\n",
    "    lengths.append(a.size)\n",
    "    mins.append(a.min())\n",
    "    maxs.append(a.max())\n",
    "lengths = np.array(lengths)\n",
    "print(\"ECG sample counts per row ‚Äî min/median/max:\", lengths.min(), np.median(lengths), lengths.max())\n",
    "print(\"Mean amplitude min/mean, max/mean:\", np.mean(mins), np.mean(maxs))\n",
    "\n",
    "# quick histogram of lengths\n",
    "plt.figure(figsize=(6,3))\n",
    "sns.histplot(lengths, bins=30)\n",
    "plt.title(\"ECG sample count distribution (per row)\")\n",
    "plt.xlabel(\"Number of samples\")\n",
    "plt.show()\n",
    "\n",
    "# plot first 3 traces (standardized)\n",
    "plt.figure(figsize=(12,4))\n",
    "for i in range(min(3, len(df))):\n",
    "    a = np.asarray(df[ecg_col].iloc[i], dtype=float)\n",
    "    a = (a - a.mean()) / (a.std() + 1e-9)\n",
    "    plt.plot(a + i*6, label=f\"rec{i}\")\n",
    "plt.title(\"First 3 ECG traces (standardized & vertically offset)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ========================================================\n",
    "# 4) Helpers: filtering, FS estimation, HRV extraction\n",
    "# ========================================================\n",
    "candidate_fs = [125, 250, 500, 360]   # common sampling rates (added 360)\n",
    "def bandpass(sig, fs, low=0.5, high=40, order=4):\n",
    "    b, a = butter(order, [low/(fs/2), high/(fs/2)], btype='band')\n",
    "    return filtfilt(b, a, sig)\n",
    "\n",
    "def notch(sig, fs, freq=50.0, q=30.0):\n",
    "    b, a = iirnotch(freq/(fs/2), q)\n",
    "    return filtfilt(b, a, sig)\n",
    "\n",
    "def estimate_fs_for_signal(arr, candidates=candidate_fs):\n",
    "    \"\"\"Choose fs that yields a physiologically plausible mean HR after R-peak detection.\"\"\"\n",
    "    arr = np.asarray(arr, dtype=float)\n",
    "    best_fs, best_hr, best_score = None, None, 1e9\n",
    "    for fs in candidates:\n",
    "        try:\n",
    "            peaks = nk.ecg_peaks(arr, sampling_rate=fs)[1].get('ECG_R_Peaks', [])\n",
    "            if len(peaks) < 3:\n",
    "                continue\n",
    "            rr = np.diff(np.array(peaks)) / fs\n",
    "            mean_hr = 60.0 / rr.mean()\n",
    "            score = 0 if (40 <= mean_hr <= 140) else abs(mean_hr - 80)\n",
    "            if score < best_score:\n",
    "                best_score = score\n",
    "                best_fs = fs\n",
    "                best_hr = mean_hr\n",
    "        except Exception:\n",
    "            continue\n",
    "    return best_fs, best_hr\n",
    "\n",
    "def extract_hrv_features_from_window(w, fs):\n",
    "    \"\"\"Compute robust HRV time-domain features from one ECG window.\"\"\"\n",
    "    out = {}\n",
    "    w = np.asarray(w, dtype=float)\n",
    "    if w.size == 0:\n",
    "        return None\n",
    "    out['max_abs'] = float(np.max(np.abs(w)))\n",
    "    if out['max_abs'] < 1e-6:\n",
    "        out['sqi'] = 0\n",
    "        return out\n",
    "    # filter (try/catch)\n",
    "    try:\n",
    "        wf = bandpass(w, fs)\n",
    "        # choose notch frequency by region if you know, default 50\n",
    "        wf = notch(wf, fs, freq=50.0)\n",
    "    except Exception:\n",
    "        wf = w.copy()\n",
    "    # R-peak detection\n",
    "    try:\n",
    "        signals, info = nk.ecg_process(wf, sampling_rate=fs)\n",
    "        rpeaks = np.array(info.get(\"ECG_R_Peaks\", []))\n",
    "    except Exception:\n",
    "        rpeaks = np.array(nk.ecg_peaks(wf, sampling_rate=fs)[1].get('ECG_R_Peaks', []))\n",
    "    if len(rpeaks) < 4:\n",
    "        out['sqi'] = 0\n",
    "        return out\n",
    "    rr_s = np.diff(rpeaks) / fs\n",
    "    rr_ms = rr_s * 1000.0\n",
    "    out['sqi'] = 1\n",
    "    out['mean_hr'] = float(60.0 / np.mean(rr_s))\n",
    "    out['sdnn'] = float(np.std(rr_ms, ddof=1))\n",
    "    if len(rr_ms) >= 3:\n",
    "        diff_rr = np.diff(rr_ms)\n",
    "        out['rmssd'] = float(np.sqrt(np.mean(diff_rr**2)))\n",
    "        out['pnn50'] = float(np.mean(np.abs(diff_rr) > 50.0))\n",
    "    else:\n",
    "        out['rmssd'] = np.nan\n",
    "        out['pnn50'] = np.nan\n",
    "    # sample entropy often fails with few RR -> skip here (we won't rely on it)\n",
    "    out['sampen'] = np.nan\n",
    "    return out\n",
    "\n",
    "# ========================================================\n",
    "# 5) Windowing & feature extraction (30s windows, 50% overlap)\n",
    "# ========================================================\n",
    "features = []\n",
    "max_recs = 200  # keep small for fast iteration; increase if you want more data\n",
    "win_s = 30\n",
    "hop_s = 15\n",
    "\n",
    "pbar = tqdm(range(min(max_recs, len(df))), desc=\"Records\")\n",
    "for idx in pbar:\n",
    "    arr = np.asarray(df[ecg_col].iloc[idx], dtype=float)\n",
    "    fs, hr = estimate_fs_for_signal(arr)\n",
    "    if fs is None:\n",
    "        # skip if we can't estimate sampling rate\n",
    "        continue\n",
    "    npts = arr.size\n",
    "    if npts < int(win_s * fs):\n",
    "        continue\n",
    "    for start in range(0, npts - int(win_s*fs) + 1, int(hop_s*fs)):\n",
    "        w = arr[start:start + int(win_s*fs)]\n",
    "        feats = extract_hrv_features_from_window(w, fs)\n",
    "        if feats is None:\n",
    "            continue\n",
    "        if feats.get('sqi', 0) == 0:\n",
    "            continue\n",
    "        rec = {\n",
    "            'rec_index': int(idx),\n",
    "            'start_sample': int(start),\n",
    "            'fs': int(fs),\n",
    "            'win_s': win_s,\n",
    "            'n_samples': int(w.size)\n",
    "        }\n",
    "        rec.update(feats)\n",
    "        if 'record_name' in df.columns:\n",
    "            rec['record_name'] = df['record_name'].iloc[idx]\n",
    "        features.append(rec)\n",
    "\n",
    "feats_df = pd.DataFrame(features)\n",
    "print(\"Extracted feature windows:\", feats_df.shape)\n",
    "display(feats_df.head())\n",
    "feats_df.to_csv('ecg_feature_windows_scg_rhc_sample.csv', index=False)\n",
    "print(\"Saved ecg_feature_windows_scg_rhc_sample.csv\")\n",
    "\n",
    "# ========================================================\n",
    "# 6) Prepare feature matrix (drop all-NaN cols like sampen)\n",
    "# ========================================================\n",
    "# Candidate features: prefer time-domain HRV features we computed\n",
    "candidate_cols = ['mean_hr', 'sdnn', 'rmssd', 'pnn50', 'sampen']\n",
    "# pick only columns that exist and have at least one non-null value\n",
    "num_cols = [c for c in candidate_cols if c in feats_df.columns and feats_df[c].notna().sum() > 0]\n",
    "print(\"Using numeric features:\", num_cols)\n",
    "\n",
    "# require at least 1 row\n",
    "if len(feats_df) == 0 or len(num_cols) == 0:\n",
    "    raise RuntimeError(\"No usable feature windows extracted. Re-check preprocessing or increase max_recs.\")\n",
    "\n",
    "# drop rows missing these features and fill any remaining NaNs with median\n",
    "feats_df_clean = feats_df.dropna(subset=num_cols).copy()\n",
    "for c in num_cols:\n",
    "    if feats_df_clean[c].isna().any():\n",
    "        median = feats_df_clean[c].median()\n",
    "        feats_df_clean[c].fillna(median, inplace=True)\n",
    "\n",
    "print(\"Feature windows after cleaning:\", feats_df_clean.shape)\n",
    "\n",
    "# ========================================================\n",
    "# 7) Unsupervised clustering -> pseudo labels\n",
    "# ========================================================\n",
    "X = feats_df_clean[num_cols].values\n",
    "scaler = StandardScaler()\n",
    "Xs = scaler.fit_transform(X)\n",
    "\n",
    "# PCA for plotting\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "Xp = pca.fit_transform(Xs)\n",
    "\n",
    "# KMeans clustering (k=3)\n",
    "k = 3\n",
    "km = KMeans(n_clusters=k, random_state=42, n_init=20)\n",
    "labels = km.fit_predict(Xs)\n",
    "sil = silhouette_score(Xs, labels) if Xs.shape[0] > 1 else float(\"nan\")\n",
    "print(\"Silhouette score (k=3):\", sil)\n",
    "\n",
    "feats_df_clean['cluster'] = labels\n",
    "plt.figure(figsize=(7,5))\n",
    "sns.scatterplot(x=Xp[:,0], y=Xp[:,1], hue=labels, palette='tab10', s=40, alpha=0.8)\n",
    "plt.title(\"PCA of HRV features colored by KMeans cluster\")\n",
    "plt.show()\n",
    "\n",
    "# cluster centers in original feature space\n",
    "centers = scaler.inverse_transform(km.cluster_centers_)\n",
    "centers_df = pd.DataFrame(centers, columns=num_cols)\n",
    "print(\"Cluster centers (HRV features):\")\n",
    "display(centers_df)\n",
    "\n",
    "# ========================================================\n",
    "# 8) Map clusters -> ordered pseudo-depth (heuristic via sdnn)\n",
    "# ========================================================\n",
    "# order clusters by sdnn descending (higher sdnn -> lighter state)\n",
    "if 'sdnn' in centers_df.columns:\n",
    "    centers_df['cluster'] = centers_df.index\n",
    "    order = centers_df.sort_values('sdnn', ascending=False)['cluster'].tolist()\n",
    "else:\n",
    "    order = list(range(k))\n",
    "cluster_to_depth = {c: i for i,c in enumerate(order)}  # 0 = light, larger = deeper (by sdnn)\n",
    "print(\"cluster_to_depth mapping:\", cluster_to_depth)\n",
    "\n",
    "feats_df_clean['pseudo_depth'] = feats_df_clean['cluster'].map(cluster_to_depth)\n",
    "\n",
    "# ========================================================\n",
    "# 9) Train supervised model (XGBoost) on pseudo labels (group split by rec_index)\n",
    "# ========================================================\n",
    "groups = feats_df_clean['rec_index'].values\n",
    "X_mat = feats_df_clean[num_cols].fillna(0.0).values\n",
    "y = feats_df_clean['pseudo_depth'].values\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, test_idx = next(gss.split(X_mat, y, groups))\n",
    "X_train, X_test = X_mat[train_idx], X_mat[test_idx]\n",
    "y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "model = xgb.XGBRegressor(n_estimators=200, max_depth=5, learning_rate=0.05, random_state=42)\n",
    "model.fit(X_train, y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=20, verbose=False)\n",
    "\n",
    "preds = model.predict(X_test)\n",
    "print(\"MAE:\", mean_absolute_error(y_test, preds))\n",
    "print(\"R2 :\", r2_score(y_test, preds))\n",
    "\n",
    "# save model and scaler\n",
    "joblib.dump(model, \"xgb_pseudo_depth_model.joblib\")\n",
    "joblib.dump(scaler, \"feature_scaler.joblib\")\n",
    "print(\"Saved model and scaler.\")\n",
    "\n",
    "# ========================================================\n",
    "# 10) Visualizations for demo\n",
    "# ========================================================\n",
    "# show example ECG window from each cluster\n",
    "print(\"Showing one example ECG window per cluster (if available):\")\n",
    "for c in sorted(feats_df_clean['cluster'].unique()):\n",
    "    subset = feats_df_clean[feats_df_clean['cluster']==c]\n",
    "    if subset.shape[0] == 0:\n",
    "        continue\n",
    "    row = subset.iloc[0]\n",
    "    rec_idx = int(row['rec_index'])\n",
    "    start = int(row['start_sample']); fs = int(row['fs'])\n",
    "    n_samples = int(row['n_samples'])\n",
    "    arr_raw = np.asarray(df[ecg_col].iloc[rec_idx], dtype=float)\n",
    "    end = min(start + n_samples, arr_raw.size)\n",
    "    w = arr_raw[start:end]\n",
    "    t = np.linspace(0, row['win_s'], w.size)\n",
    "    plt.figure(figsize=(9,2))\n",
    "    plt.plot(t, (w - w.mean())/(w.std()+1e-9))\n",
    "    plt.title(f\"Cluster {c} example (rec {rec_idx}, fs {fs})\")\n",
    "    plt.xlabel(\"seconds\")\n",
    "    plt.show()\n",
    "\n",
    "# show cluster timeline for one recording (choose a rec with multiple windows)\n",
    "example_rec = int(feats_df_clean['rec_index'].mode()[0])\n",
    "rec_windows = feats_df_clean[feats_df_clean['rec_index']==example_rec].sort_values('start_sample')\n",
    "if rec_windows.shape[0] > 0:\n",
    "    times = rec_windows['start_sample'] / rec_windows['fs']\n",
    "    plt.figure(figsize=(10,2))\n",
    "    plt.plot(times, rec_windows['pseudo_depth'], marker='o')\n",
    "    plt.gca().invert_yaxis()  # lower plot = lighter depth (optional)\n",
    "    plt.title(f\"Pseudo-depth timeline for record {example_rec}\")\n",
    "    plt.xlabel(\"seconds from start\")\n",
    "    plt.ylabel(\"pseudo_depth (0=light)\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No windows found for example record.\")\n",
    "\n",
    "# ========================================================\n",
    "# 11) How to replace pseudo labels with real labels later\n",
    "# ========================================================\n",
    "print(\"\"\"\n",
    "DONE: pipeline complete.\n",
    "\n",
    "Notes for next steps:\n",
    "- When you obtain clinical labels (GCS/BIS) for each window:\n",
    "  * create feats_df_clean['true_label'] aligned by rec_index & start time,\n",
    "  * replace y with feats_df_clean['true_label'] and retrain model (same code).\n",
    "- This notebook currently uses pseudo labels (KMeans on HRV). It's a demo/prototype.\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a669bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T16:06:04.926077Z",
     "iopub.status.busy": "2025-09-02T16:06:04.925683Z",
     "iopub.status.idle": "2025-09-02T16:06:40.463008Z",
     "shell.execute_reply": "2025-09-02T16:06:40.460267Z",
     "shell.execute_reply.started": "2025-09-02T16:06:04.926030Z"
    },
    "papermill": {
     "duration": 0.005645,
     "end_time": "2025-09-02T21:15:11.935410",
     "exception": false,
     "start_time": "2025-09-02T21:15:11.929765",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ========================================================\n",
    "# 0) Installs (run once at top)\n",
    "# ========================================================\n",
    "!pip install --quiet neurokit2 xgboost joblib seaborn tqdm streamlit\n",
    "\n",
    "# ========================================================\n",
    "# 1) Imports & constants\n",
    "# ========================================================\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (9,4)\n",
    "\n",
    "# Modeling imports\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "\n",
    "# Signal processing\n",
    "import neurokit2 as nk\n",
    "from scipy.signal import butter, filtfilt, iirnotch\n",
    "\n",
    "# ========================================================\n",
    "# 2) Load parquet and inspect\n",
    "# ========================================================\n",
    "PQ_PATH = \"/kaggle/input/physionet-ecg/scg_rhc_f16.parquet\"\n",
    "assert os.path.exists(PQ_PATH), f\"Parquet file not found at {PQ_PATH}\"\n",
    "df = pd.read_parquet(PQ_PATH)\n",
    "print(\"Loaded parquet:\", PQ_PATH)\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "\n",
    "preferred = [\"ECG_lead_II\", \"ECG_lead_ii\", \"ECG_lead_II \", \"ECG_lead_I\", \"ecg_lead_ii\"]\n",
    "ecg_col = None\n",
    "for p in preferred:\n",
    "    if p in df.columns:\n",
    "        ecg_col = p\n",
    "        break\n",
    "if ecg_col is None:\n",
    "    for c in df.columns:\n",
    "        if 'ecg' in c.lower():\n",
    "            ecg_col = c\n",
    "            break\n",
    "if ecg_col is None:\n",
    "    raise RuntimeError(\"No ECG column found in the parquet file.\")\n",
    "print(\"Using ECG column:\", ecg_col)\n",
    "\n",
    "# ========================================================\n",
    "# 3) Quick diagnostics\n",
    "# ========================================================\n",
    "lengths = [len(np.asarray(arr, float)) for arr in df[ecg_col]]\n",
    "print(\"ECG sample counts per row ‚Äî min/median/max:\",\n",
    "      np.min(lengths), np.median(lengths), np.max(lengths))\n",
    "\n",
    "# ========================================================\n",
    "# 4) Helpers: filtering, FS estimation, HRV extraction\n",
    "# ========================================================\n",
    "candidate_fs = [125, 250, 360, 500]\n",
    "\n",
    "def bandpass(sig, fs, low=0.5, high=40, order=4):\n",
    "    b, a = butter(order, [low/(fs/2), high/(fs/2)], btype='band')\n",
    "    return filtfilt(b, a, sig)\n",
    "\n",
    "def notch(sig, fs, freq=50.0, q=30.0):\n",
    "    b, a = iirnotch(freq/(fs/2), q)\n",
    "    return filtfilt(b, a, sig)\n",
    "\n",
    "def estimate_fs_for_signal(arr, candidates=candidate_fs):\n",
    "    arr = np.asarray(arr, dtype=float)\n",
    "    best_fs, best_score = None, 1e9\n",
    "    for fs in candidates:\n",
    "        try:\n",
    "            peaks = nk.ecg_peaks(arr, sampling_rate=fs)[1].get('ECG_R_Peaks', [])\n",
    "            if len(peaks) < 3:\n",
    "                continue\n",
    "            rr = np.diff(peaks) / fs\n",
    "            mean_hr = 60.0 / rr.mean()\n",
    "            score = 0 if (40 <= mean_hr <= 140) else abs(mean_hr - 80)\n",
    "            if score < best_score:\n",
    "                best_score = score\n",
    "                best_fs = fs\n",
    "        except Exception:\n",
    "            continue\n",
    "    return best_fs, None\n",
    "\n",
    "# üîπ NEW: RR diagnostics\n",
    "def rr_diagnostics(rr_ms):\n",
    "    flags = {}\n",
    "    if len(rr_ms) < 3:\n",
    "        flags['valid'] = False\n",
    "        return flags\n",
    "    mean_rr = np.mean(rr_ms)\n",
    "    cv_rr = np.std(rr_ms) / mean_rr if mean_rr > 0 else np.inf\n",
    "    flags['valid'] = True\n",
    "    flags['mean_rr_ms'] = mean_rr\n",
    "    flags['cv_rr'] = cv_rr\n",
    "    flags['too_fast'] = mean_rr < 300   # HR > 200 bpm\n",
    "    flags['too_slow'] = mean_rr > 2000  # HR < 30 bpm\n",
    "    flags['irregular'] = cv_rr > 0.3\n",
    "    return flags\n",
    "\n",
    "# üîπ UPDATED: feature extraction with LF/HF + RR flags\n",
    "def extract_hrv_features_from_window(w, fs):\n",
    "    out = {}\n",
    "    w = np.asarray(w, dtype=float)\n",
    "    if w.size == 0:\n",
    "        return None\n",
    "    out['max_abs'] = float(np.max(np.abs(w)))\n",
    "    if out['max_abs'] < 1e-6:\n",
    "        out['sqi'] = 0\n",
    "        return out\n",
    "    try:\n",
    "        wf = bandpass(w, fs)\n",
    "        wf = notch(wf, fs, freq=50.0)\n",
    "    except Exception:\n",
    "        wf = w.copy()\n",
    "    try:\n",
    "        signals, info = nk.ecg_process(wf, sampling_rate=fs)\n",
    "        rpeaks = np.array(info.get(\"ECG_R_Peaks\", []))\n",
    "    except Exception:\n",
    "        rpeaks = np.array(nk.ecg_peaks(wf, sampling_rate=fs)[1].get('ECG_R_Peaks', []))\n",
    "    if len(rpeaks) < 4:\n",
    "        out['sqi'] = 0\n",
    "        return out\n",
    "    rr_s = np.diff(rpeaks) / fs\n",
    "    rr_ms = rr_s * 1000.0\n",
    "    out['sqi'] = 1\n",
    "    out['mean_hr'] = float(60.0 / np.mean(rr_s))\n",
    "    out['sdnn'] = float(np.std(rr_ms, ddof=1))\n",
    "    if len(rr_ms) >= 3:\n",
    "        diff_rr = np.diff(rr_ms)\n",
    "        out['rmssd'] = float(np.sqrt(np.mean(diff_rr**2)))\n",
    "        out['pnn50'] = float(np.mean(np.abs(diff_rr) > 50.0))\n",
    "    else:\n",
    "        out['rmssd'] = np.nan\n",
    "        out['pnn50'] = np.nan\n",
    "    # üîπ LF/HF ratio\n",
    "    try:\n",
    "        hrv_freq = nk.hrv_frequency(rpeaks, sampling_rate=fs, show=False)\n",
    "        out['lf_hf'] = float(hrv_freq['HRV_LFHF'].iloc[0])\n",
    "    except Exception:\n",
    "        out['lf_hf'] = np.nan\n",
    "    # üîπ RR flags\n",
    "    flags = rr_diagnostics(rr_ms)\n",
    "    out['rr_valid'] = int(flags.get('valid', 0))\n",
    "    out['rr_irregular'] = int(flags.get('irregular', 0))\n",
    "    return out\n",
    "\n",
    "# ========================================================\n",
    "# 5) Windowing & feature extraction\n",
    "# ========================================================\n",
    "features = []\n",
    "max_recs = 200\n",
    "win_s = 30\n",
    "hop_s = 15\n",
    "\n",
    "for idx in tqdm(range(min(max_recs, len(df))), desc=\"Records\"):\n",
    "    arr = np.asarray(df[ecg_col].iloc[idx], dtype=float)\n",
    "    fs, _ = estimate_fs_for_signal(arr)\n",
    "    if fs is None:\n",
    "        continue\n",
    "    npts = arr.size\n",
    "    if npts < int(win_s * fs):\n",
    "        continue\n",
    "    for start in range(0, npts - int(win_s*fs) + 1, int(hop_s*fs)):\n",
    "        w = arr[start:start + int(win_s*fs)]\n",
    "        feats = extract_hrv_features_from_window(w, fs)\n",
    "        if feats is None or feats.get('sqi', 0) == 0:\n",
    "            continue\n",
    "        rec = {\n",
    "            'rec_index': int(idx),\n",
    "            'start_sample': int(start),\n",
    "            'fs': int(fs),\n",
    "            'win_s': win_s,\n",
    "            'n_samples': int(w.size)\n",
    "        }\n",
    "        rec.update(feats)\n",
    "        if 'record_name' in df.columns:\n",
    "            rec['record_name'] = df['record_name'].iloc[idx]\n",
    "        features.append(rec)\n",
    "\n",
    "feats_df = pd.DataFrame(features)\n",
    "print(\"Extracted feature windows:\", feats_df.shape)\n",
    "feats_df.to_csv('ecg_feature_windows_scg_rhc_sample.csv', index=False)\n",
    "\n",
    "# ========================================================\n",
    "# 6) Feature prep\n",
    "# ========================================================\n",
    "candidate_cols = ['mean_hr', 'sdnn', 'rmssd', 'pnn50', 'lf_hf']\n",
    "num_cols = [c for c in candidate_cols if c in feats_df.columns and feats_df[c].notna().sum() > 0]\n",
    "print(\"Using numeric features:\", num_cols)\n",
    "feats_df_clean = feats_df.dropna(subset=num_cols).copy()\n",
    "for c in num_cols:\n",
    "    if feats_df_clean[c].isna().any():\n",
    "        feats_df_clean[c].fillna(feats_df_clean[c].median(), inplace=True)\n",
    "\n",
    "# ========================================================\n",
    "# 7) Clustering\n",
    "# ========================================================\n",
    "X = feats_df_clean[num_cols].values\n",
    "scaler = StandardScaler()\n",
    "Xs = scaler.fit_transform(X)\n",
    "k = 3\n",
    "km = KMeans(n_clusters=k, random_state=42, n_init=20)\n",
    "labels = km.fit_predict(Xs)\n",
    "sil = silhouette_score(Xs, labels)\n",
    "print(\"Silhouette score (k=3):\", sil)\n",
    "feats_df_clean['cluster'] = labels\n",
    "centers_df = pd.DataFrame(scaler.inverse_transform(km.cluster_centers_), columns=num_cols)\n",
    "print(\"Cluster centers:\\n\", centers_df)\n",
    "\n",
    "# ========================================================\n",
    "# 8) Pseudo-depth mapping\n",
    "# ========================================================\n",
    "order = centers_df.sort_values('sdnn', ascending=False).index.tolist()\n",
    "cluster_to_depth = {c: i for i, c in enumerate(order)}\n",
    "feats_df_clean['pseudo_depth'] = feats_df_clean['cluster'].map(cluster_to_depth)\n",
    "\n",
    "# ========================================================\n",
    "# 9) Train XGBoost regressor\n",
    "# ========================================================\n",
    "groups = feats_df_clean['rec_index'].values\n",
    "X_mat = feats_df_clean[num_cols].values\n",
    "y = feats_df_clean['pseudo_depth'].values\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, test_idx = next(gss.split(X_mat, y, groups))\n",
    "X_train, X_test = X_mat[train_idx], X_mat[test_idx]\n",
    "y_train, y_test = y[train_idx], y[test_idx]\n",
    "model = xgb.XGBRegressor(n_estimators=200, max_depth=5, learning_rate=0.05, random_state=42)\n",
    "model.fit(X_train, y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=20, verbose=False)\n",
    "preds = model.predict(X_test)\n",
    "print(\"MAE:\", mean_absolute_error(y_test, preds))\n",
    "print(\"R2 :\", r2_score(y_test, preds))\n",
    "\n",
    "\n",
    "# ========================================================\n",
    "# 9b) Save Model and Scaler\n",
    "# ========================================================\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Create an \"outputs\" folder (Kaggle recommends this)\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "# Save trained model\n",
    "model_path = \"outputs/ecg_sleep_model.pkl\"\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "# Save scaler\n",
    "scaler_path = \"outputs/scaler.pkl\"\n",
    "joblib.dump(scaler, scaler_path)\n",
    "\n",
    "print(f\"‚úÖ Model saved at: {model_path}\")\n",
    "print(f\"‚úÖ Scaler saved at: {scaler_path}\")\n",
    "\n",
    "# Check files\n",
    "print(\"Files in outputs folder:\", os.listdir(\"outputs\"))\n",
    "\n",
    "\n",
    "# ========================================================\n",
    "# 10) Streamlit demo (save as app.py in same dir)\n",
    "# ========================================================\n",
    "demo_code = r'''\n",
    "# app.py - Streamlit app for ECG -> pseudo-depth timeline (copy/paste entire file)\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import neurokit2 as nk\n",
    "import os\n",
    "\n",
    "st.set_page_config(layout=\"wide\", page_title=\"UPACS ‚Äî ECG pseudo-depth demo\")\n",
    "\n",
    "# ------------------------\n",
    "# Helpers\n",
    "# ------------------------\n",
    "@st.cache_resource\n",
    "def load_artifacts(model_path=\"ecg_sleep_model.pkl\", scaler_path=\"scaler.pkl\"):\n",
    "    \"\"\"Load model & scaler; return (model, scaler) or (None,None) with error msg.\"\"\"\n",
    "    try:\n",
    "        model = joblib.load(model_path)\n",
    "        scaler = joblib.load(scaler_path)\n",
    "        return model, scaler, None\n",
    "    except Exception as e:\n",
    "        return None, None, str(e)\n",
    "\n",
    "def infer_sampling_rate_from_timecol(time_arr):\n",
    "    t = np.asarray(time_arr, dtype=float)\n",
    "    if t.size < 3: \n",
    "        return None\n",
    "    dt = np.diff(t)\n",
    "    median_dt = np.median(dt)\n",
    "    if median_dt <= 0:\n",
    "        return None\n",
    "    fs = int(round(1.0 / median_dt))\n",
    "    return fs\n",
    "\n",
    "def compute_hrv_features(ecg_signal, fs):\n",
    "    \"\"\"\n",
    "    Compute the HRV time-domain features used by the model:\n",
    "      mean_hr (bpm), sdnn (ms), rmssd (ms), pnn50 (fraction)\n",
    "    Return dict or None if detection failed.\n",
    "    \"\"\"\n",
    "    sig = np.asarray(ecg_signal, dtype=float)\n",
    "    if sig.size < 10:\n",
    "        return None\n",
    "    # Clean ECG (NeuroKit2)\n",
    "    try:\n",
    "        cleaned = nk.ecg_clean(sig, sampling_rate=fs)\n",
    "        signals, info = nk.ecg_process(cleaned, sampling_rate=fs)\n",
    "        rpeaks = np.array(info.get(\"ECG_R_Peaks\", []))\n",
    "    except Exception:\n",
    "        # fallback simple peak detection\n",
    "        rpeaks = np.array(nk.ecg_peaks(sig, sampling_rate=fs)[1].get(\"ECG_R_Peaks\", []))\n",
    "    if rpeaks.size < 4:\n",
    "        return None\n",
    "    rr_s = np.diff(rpeaks) / fs\n",
    "    if rr_s.size < 2:\n",
    "        return None\n",
    "    rr_ms = rr_s * 1000.0\n",
    "    mean_hr = float(60.0 / np.mean(rr_s))\n",
    "    sdnn = float(np.std(rr_ms, ddof=1))\n",
    "    diff_rr = np.diff(rr_ms)\n",
    "    rmssd = float(np.sqrt(np.mean(diff_rr**2))) if diff_rr.size > 0 else np.nan\n",
    "    pnn50 = float(np.mean(np.abs(diff_rr) > 50.0)) if diff_rr.size > 0 else np.nan\n",
    "    return {\"mean_hr\": mean_hr, \"sdnn\": sdnn, \"rmssd\": rmssd, \"pnn50\": pnn50}\n",
    "\n",
    "# ------------------------\n",
    "# Load model + scaler\n",
    "# ------------------------\n",
    "model, scaler, load_err = load_artifacts()\n",
    "if load_err:\n",
    "    st.warning(\"Could not load model/scaler automatically. Make sure files exist in the app folder:\")\n",
    "    st.info(\"Looking for: ecg_sleep_model.pkl and scaler.pkl\")\n",
    "    st.error(load_err)\n",
    "    # still let user upload ECG to inspect waveform even without model\n",
    "else:\n",
    "    st.success(\"Model and scaler loaded successfully.\")\n",
    "\n",
    "# Determine features order expected by scaler/model\n",
    "# Preferred: use model.feature_names_in_ if available; otherwise assume the trained HRV set\n",
    "default_feature_order = [\"mean_hr\", \"sdnn\", \"rmssd\", \"pnn50\"]\n",
    "\n",
    "if model is not None and hasattr(model, \"feature_names_in_\"):\n",
    "    expected_features = list(model.feature_names_in_)\n",
    "else:\n",
    "    expected_features = default_feature_order.copy()\n",
    "\n",
    "# Ensure scaler.feature count matches expected features length (fallback if mismatch)\n",
    "if scaler is not None:\n",
    "    n_expected = scaler.n_features_in_\n",
    "    if n_expected != len(expected_features):\n",
    "        # take first n_expected from default (most likely case)\n",
    "        expected_features = default_feature_order[:n_expected]\n",
    "\n",
    "st.title(\"UPACS ‚Äî ECG ‚Üí Pseudo-Depth Demo\")\n",
    "st.write(\"Upload an ECG CSV (columns: `ecg` and optional `time`) or use sample ECG to test.\")\n",
    "\n",
    "# ------------------------\n",
    "# File uploader or sample button\n",
    "# ------------------------\n",
    "col1, col2 = st.columns([3,1])\n",
    "with col2:\n",
    "    use_sample = st.button(\"Use sample synthetic ECG (10s)\")\n",
    "\n",
    "uploaded_file = st.file_uploader(\"Upload ECG CSV file (columns: 'ecg' and optional 'time')\", type=[\"csv\"])\n",
    "\n",
    "# If no file uploaded and user clicks sample, create sample\n",
    "if uploaded_file is None and use_sample:\n",
    "    # Generate synthetic ECG-like signal (demo only)\n",
    "    n_samples = 2000\n",
    "    time = np.linspace(0, 20, n_samples)  # 20 seconds\n",
    "    ecg_signal = 0.6*np.sin(1.7*np.pi*time) + 0.3*np.sin(3.5*np.pi*time) + 0.05*np.random.randn(n_samples)\n",
    "    df = pd.DataFrame({\"time\": time, \"ecg\": ecg_signal})\n",
    "    st.success(\"Loaded sample synthetic ECG (20 s).\")\n",
    "elif uploaded_file is not None:\n",
    "    try:\n",
    "        df = pd.read_csv(uploaded_file)\n",
    "    except Exception as e:\n",
    "        st.error(f\"Failed to read uploaded file: {e}\")\n",
    "        st.stop()\n",
    "else:\n",
    "    st.info(\"Upload a CSV or click 'Use sample' to proceed.\")\n",
    "    st.stop()\n",
    "\n",
    "# show preview\n",
    "st.subheader(\"Uploaded ECG preview\")\n",
    "st.dataframe(df.head(10))\n",
    "\n",
    "# ------------------------\n",
    "# Determine ECG column and sampling rate\n",
    "# ------------------------\n",
    "# choose ecg column (case-insensitive)\n",
    "if \"ecg\" in df.columns:\n",
    "    ecg_col = \"ecg\"\n",
    "else:\n",
    "    # try other likely names\n",
    "    candidates = [c for c in df.columns if \"ecg\" in c.lower() or \"signal\" in c.lower()]\n",
    "    if len(candidates) > 0:\n",
    "        ecg_col = candidates[0]\n",
    "    else:\n",
    "        # fallback to numeric column with most rows\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        if len(numeric_cols) == 0:\n",
    "            st.error(\"No numeric columns found in uploaded file. Provide a CSV with ECG samples in a column named 'ecg'.\")\n",
    "            st.stop()\n",
    "        ecg_col = numeric_cols[0]\n",
    "st.write(f\"Using ECG column: `{ecg_col}`\")\n",
    "\n",
    "# infer sampling rate if time column exists\n",
    "fs = None\n",
    "if \"time\" in df.columns:\n",
    "    fs = infer_sampling_rate_from_timecol(df[\"time\"].values)\n",
    "    if fs is not None:\n",
    "        st.write(f\"Inferred sampling rate from `time` column: {fs} Hz\")\n",
    "    else:\n",
    "        st.info(\"Could not infer sampling rate from `time` column.\")\n",
    "# interactive override\n",
    "fs_input = st.number_input(\"Sampling rate (Hz) ‚Äî if not inferred, set here\", min_value=10, max_value=2000, value=fs or 250)\n",
    "fs = int(fs_input)\n",
    "\n",
    "# ------------------------\n",
    "# User options for windowing\n",
    "# ------------------------\n",
    "st.sidebar.header(\"Windowing options\")\n",
    "win_s = st.sidebar.number_input(\"Window length (seconds)\", min_value=5, max_value=120, value=30, step=5)\n",
    "hop_s = st.sidebar.number_input(\"Hop length (seconds)\", min_value=1, max_value=60, value=5, step=1)\n",
    "\n",
    "win_samples = int(win_s * fs)\n",
    "hop_samples = int(hop_s * fs)\n",
    "\n",
    "ecg_array = np.asarray(df[ecg_col].values, dtype=float)\n",
    "npts = ecg_array.size\n",
    "total_time = npts / fs\n",
    "\n",
    "st.write(f\"Signal length: {npts} samples ({total_time:.1f} s) ‚Äî window: {win_s}s hop: {hop_s}s\")\n",
    "\n",
    "# ------------------------\n",
    "# Sliding window predictions\n",
    "# ------------------------\n",
    "starts = list(range(0, max(1, npts - win_samples + 1), hop_samples))\n",
    "times = []\n",
    "preds = []\n",
    "valid_windows = 0\n",
    "\n",
    "for start in starts:\n",
    "    w = ecg_array[start:start + win_samples]\n",
    "    feats = compute_hrv_features(w, fs)\n",
    "    if feats is None:\n",
    "        continue\n",
    "    # create feature vector in expected order\n",
    "    X_vec = [feats.get(f, 0.0) for f in expected_features]\n",
    "    try:\n",
    "        X_scaled = scaler.transform([X_vec]) if scaler is not None else np.array([X_vec])\n",
    "        if model is not None:\n",
    "            pred = model.predict(X_scaled)[0]\n",
    "        else:\n",
    "            pred = X_vec[0]  # fallback show HR if no model\n",
    "    except Exception as e:\n",
    "        st.warning(f\"Scaling/prediction error for window at {start/fs:.1f}s: {e}\")\n",
    "        continue\n",
    "    preds.append(pred)\n",
    "    times.append((start + win_samples/2) / fs)  # use window center time\n",
    "    valid_windows += 1\n",
    "\n",
    "st.write(f\"Valid windows (with detectable R-peaks): {valid_windows} / {len(starts)}\")\n",
    "\n",
    "if len(preds) == 0:\n",
    "    st.warning(\"No valid windows found (R-peaks not detected). Try increasing window length or checking sampling rate.\")\n",
    "# ------------------------\n",
    "# Plotting\n",
    "# ------------------------\n",
    "fig, axes = plt.subplots(2, 1, figsize=(10, 6), sharex=True)\n",
    "# (1) raw ECG (downsample for display if long)\n",
    "display_len = min(npts, 5000)\n",
    "step = max(1, npts // display_len)\n",
    "axes[0].plot(np.arange(0, npts, step)/fs, ecg_array[::step], linewidth=0.7)\n",
    "axes[0].set_ylabel(\"ECG amplitude\")\n",
    "axes[0].set_title(\"Raw ECG signal\")\n",
    "\n",
    "# (2) pseudo-depth timeline / predictions\n",
    "if len(preds) > 0:\n",
    "    axes[1].plot(times, preds, marker='o', linestyle='-')\n",
    "    axes[1].set_ylabel(\"Pseudo-depth (model output)\")\n",
    "    axes[1].set_xlabel(\"Time (s)\")\n",
    "    axes[1].set_title(\"Pseudo-depth timeline (per window center)\")\n",
    "else:\n",
    "    axes[1].text(0.5, 0.5, \"No predictions ‚Äî no valid windows\", ha='center', va='center', fontsize=12)\n",
    "    axes[1].set_xlabel(\"Time (s)\")\n",
    "\n",
    "st.pyplot(fig)\n",
    "\n",
    "# ------------------------\n",
    "# Show a representative table of features for first few valid windows\n",
    "# ------------------------\n",
    "if valid_windows > 0:\n",
    "    feat_rows = []\n",
    "    for start in starts:\n",
    "        w = ecg_array[start:start + win_samples]\n",
    "        feats = compute_hrv_features(w, fs)\n",
    "        if feats is None:\n",
    "            continue\n",
    "        row = {\"start_s\": start / fs}\n",
    "        for k, v in feats.items():\n",
    "            row[k] = v\n",
    "        feat_rows.append(row)\n",
    "        if len(feat_rows) >= 10:\n",
    "            break\n",
    "    st.subheader(\"Example window HRV features (first valid windows)\")\n",
    "    st.dataframe(pd.DataFrame(feat_rows).reset_index(drop=True)) '''\n",
    "\n",
    "with open(\"app.py\", \"w\") as f:\n",
    "    f.write(demo_code)\n",
    "\n",
    "print(\"‚úÖ Notebook complete. Run `streamlit run app.py` in a terminal to launch the demo.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed4db5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T19:45:20.447493Z",
     "iopub.status.busy": "2025-09-02T19:45:20.447001Z",
     "iopub.status.idle": "2025-09-02T19:46:16.085158Z",
     "shell.execute_reply": "2025-09-02T19:46:16.084054Z",
     "shell.execute_reply.started": "2025-09-02T19:45:20.447458Z"
    },
    "papermill": {
     "duration": 0.005756,
     "end_time": "2025-09-02T21:15:11.947236",
     "exception": false,
     "start_time": "2025-09-02T21:15:11.941480",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ========================================================\n",
    "# 0) Installs\n",
    "# ========================================================\n",
    "!pip install --quiet neurokit2 xgboost joblib seaborn tqdm streamlit torch torchvision torchaudio\n",
    "\n",
    "# ========================================================\n",
    "# 1) Imports & constants\n",
    "# ========================================================\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (9,4)\n",
    "\n",
    "# ML + preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "\n",
    "# Signal processing\n",
    "import neurokit2 as nk\n",
    "from scipy.signal import butter, filtfilt, iirnotch\n",
    "\n",
    "# Deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# ========================================================\n",
    "# 2) Load parquet and pick ECG lead\n",
    "# ========================================================\n",
    "PQ_PATH = \"/kaggle/input/physionet-ecg/scg_rhc_f16.parquet\"\n",
    "df = pd.read_parquet(PQ_PATH)\n",
    "print(\"Loaded parquet:\", PQ_PATH, \"Shape:\", df.shape)\n",
    "\n",
    "preferred = [\"ECG_lead_II\", \"ECG_lead_ii\", \"ECG_lead_I\"]\n",
    "ecg_col = None\n",
    "for p in preferred:\n",
    "    if p in df.columns:\n",
    "        ecg_col = p\n",
    "        break\n",
    "if ecg_col is None:\n",
    "    for c in df.columns:\n",
    "        if \"ecg\" in c.lower():\n",
    "            ecg_col = c\n",
    "            break\n",
    "if ecg_col is None:\n",
    "    raise RuntimeError(\"No ECG column found!\")\n",
    "print(\"Using ECG column:\", ecg_col)\n",
    "\n",
    "# ========================================================\n",
    "# 3) Helpers (filtering, HRV)\n",
    "# ========================================================\n",
    "def bandpass(sig, fs, low=0.5, high=40, order=4):\n",
    "    b, a = butter(order, [low/(fs/2), high/(fs/2)], btype=\"band\")\n",
    "    return filtfilt(b, a, sig)\n",
    "\n",
    "def notch(sig, fs, freq=50.0, q=30.0):\n",
    "    b, a = iirnotch(freq/(fs/2), q)\n",
    "    return filtfilt(b, a, sig)\n",
    "\n",
    "def estimate_fs_for_signal(arr, candidates=[125,250,360,500]):\n",
    "    best_fs, best_score = None, 1e9\n",
    "    for fs in candidates:\n",
    "        try:\n",
    "            peaks = nk.ecg_peaks(arr, sampling_rate=fs)[1].get(\"ECG_R_Peaks\", [])\n",
    "            if len(peaks) < 3: continue\n",
    "            rr = np.diff(peaks)/fs\n",
    "            mean_hr = 60.0/rr.mean()\n",
    "            score = 0 if (40<=mean_hr<=140) else abs(mean_hr-80)\n",
    "            if score < best_score:\n",
    "                best_score, best_fs = score, fs\n",
    "        except: continue\n",
    "    return best_fs, None\n",
    "\n",
    "def extract_hrv_features_from_window(w, fs):\n",
    "    out = {}\n",
    "    try:\n",
    "        wf = bandpass(w, fs)\n",
    "        wf = notch(wf, fs, 50.0)\n",
    "    except: wf = w\n",
    "    try:\n",
    "        signals, info = nk.ecg_process(wf, sampling_rate=fs)\n",
    "        rpeaks = np.array(info[\"ECG_R_Peaks\"])\n",
    "    except: return None\n",
    "    if len(rpeaks)<4: return None\n",
    "    rr_s = np.diff(rpeaks)/fs\n",
    "    rr_ms = rr_s*1000\n",
    "    out[\"mean_hr\"] = 60.0/rr_s.mean()\n",
    "    out[\"sdnn\"] = np.std(rr_ms, ddof=1)\n",
    "    diff_rr = np.diff(rr_ms)\n",
    "    out[\"rmssd\"] = np.sqrt(np.mean(diff_rr**2))\n",
    "    out[\"pnn50\"] = np.mean(np.abs(diff_rr)>50.0)\n",
    "    return out\n",
    "\n",
    "# ========================================================\n",
    "# 4) Windowing & feature extraction\n",
    "# ========================================================\n",
    "features, raw_windows = [], []\n",
    "max_recs = 200\n",
    "win_s, hop_s = 30, 15\n",
    "\n",
    "lengths = [len(np.asarray(arr,float)) for arr in df[ecg_col]]\n",
    "print(\"ECG lengths per row ‚Äî min/median/max:\", np.min(lengths), np.median(lengths), np.max(lengths))\n",
    "\n",
    "for idx in tqdm(range(min(max_recs,len(df))), desc=\"Records\"):\n",
    "    arr = np.asarray(df[ecg_col].iloc[idx], float)\n",
    "    fs,_ = estimate_fs_for_signal(arr)\n",
    "    if fs is None: continue\n",
    "    npts = arr.size\n",
    "    if npts < int(win_s*fs): continue\n",
    "    for start in range(0, npts-int(win_s*fs)+1, int(hop_s*fs)):\n",
    "        w = arr[start:start+int(win_s*fs)]\n",
    "        feats = extract_hrv_features_from_window(w, fs)\n",
    "        if feats is None: continue\n",
    "        rec = {\"rec_index\": idx, \"fs\": fs, \"n_samples\": len(w)}\n",
    "        rec.update(feats)\n",
    "        features.append(rec)\n",
    "        raw_windows.append(w)\n",
    "\n",
    "feats_df = pd.DataFrame(features)\n",
    "print(\"Extracted windows:\", feats_df.shape)\n",
    "\n",
    "# ========================================================\n",
    "# 5) Clustering ‚Üí pseudo labels\n",
    "# ========================================================\n",
    "num_cols = [\"mean_hr\",\"sdnn\",\"rmssd\",\"pnn50\"]\n",
    "feats_df = feats_df.dropna(subset=num_cols)\n",
    "raw_windows = [raw_windows[i] for i in feats_df.index]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "Xs = scaler.fit_transform(feats_df[num_cols])\n",
    "\n",
    "k=3\n",
    "km = KMeans(n_clusters=k, random_state=SEED, n_init=20)\n",
    "labels = km.fit_predict(Xs)\n",
    "feats_df[\"cluster\"] = labels\n",
    "centers = pd.DataFrame(scaler.inverse_transform(km.cluster_centers_),columns=num_cols)\n",
    "order = centers.sort_values(\"sdnn\", ascending=False).index\n",
    "mapping = {c:i for i,c in enumerate(order)}\n",
    "feats_df[\"pseudo_depth\"] = feats_df[\"cluster\"].map(mapping)\n",
    "\n",
    "# ========================================================\n",
    "# 6) Baseline XGB\n",
    "# ========================================================\n",
    "groups = feats_df[\"rec_index\"].values\n",
    "X = feats_df[num_cols].values\n",
    "y = feats_df[\"pseudo_depth\"].values\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "train_idx,test_idx = next(GroupShuffleSplit(n_splits=1,test_size=0.2,random_state=SEED).split(X,y,groups))\n",
    "X_train,X_test,y_train,y_test = X[train_idx],X[test_idx],y[train_idx],y[test_idx]\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(n_estimators=300, max_depth=5, learning_rate=0.05)\n",
    "xgb_model.fit(X_train,y_train,eval_set=[(X_test,y_test)],verbose=False)\n",
    "print(\"[XGB] MAE:\",mean_absolute_error(y_test,xgb_model.predict(X_test)))\n",
    "\n",
    "# ========================================================\n",
    "# 7) Dataset with fixed-length signals\n",
    "# ========================================================\n",
    "def fix_length(sig, target_len):\n",
    "    sig = np.asarray(sig,float)\n",
    "    if len(sig) > target_len: \n",
    "        return sig[:target_len]\n",
    "    elif len(sig) < target_len:\n",
    "        return np.pad(sig,(0,target_len-len(sig)),\"constant\")\n",
    "    return sig\n",
    "\n",
    "class ECGHRVDataset(Dataset):\n",
    "    def __init__(self, wins, hrv, labels, target_len):\n",
    "        self.wins, self.hrv, self.y = wins, hrv, labels\n",
    "        self.target_len = target_len\n",
    "    def __len__(self): return len(self.y)\n",
    "    def __getitem__(self, idx):\n",
    "        sig = fix_length(self.wins[idx], self.target_len)\n",
    "        sig = (sig - sig.mean())/(sig.std()+1e-8)\n",
    "        sig = torch.from_numpy(sig.astype(np.float32)).unsqueeze(0) # [1,T]\n",
    "        hrv = torch.from_numpy(self.hrv[idx].astype(np.float32))\n",
    "        y = torch.tensor(self.y[idx],dtype=torch.float32)\n",
    "        return sig,hrv,y\n",
    "\n",
    "max_len = max(len(w) for w in raw_windows)  # unify length\n",
    "train_ds = ECGHRVDataset([raw_windows[i] for i in train_idx],X[train_idx],y[train_idx],max_len)\n",
    "test_ds  = ECGHRVDataset([raw_windows[i] for i in test_idx],X[test_idx],y[test_idx],max_len)\n",
    "train_loader = DataLoader(train_ds,batch_size=32,shuffle=True)\n",
    "test_loader  = DataLoader(test_ds,batch_size=32,shuffle=False)\n",
    "\n",
    "# ========================================================\n",
    "# 8) CNN-LSTM Fusion model\n",
    "# ========================================================\n",
    "class CNNLSTM_Fusion(nn.Module):\n",
    "    def __init__(self, hrv_dim, lstm_hidden=64, emb_dim=64):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(1,16,7,padding=3)\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        self.conv2 = nn.Conv1d(16,32,5,padding=2)\n",
    "        self.conv3 = nn.Conv1d(32,64,5,padding=2)\n",
    "        self.lstm = nn.LSTM(64,lstm_hidden,batch_first=True,bidirectional=True)\n",
    "        self.emb = nn.Linear(2*lstm_hidden, emb_dim)\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(emb_dim+hrv_dim,64), nn.ReLU(), nn.Linear(64,1)\n",
    "        )\n",
    "    def forward(self,x_sig,x_hrv):\n",
    "        x = self.pool(torch.relu(self.conv1(x_sig)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = self.pool(torch.relu(self.conv3(x)))\n",
    "        x = x.permute(0,2,1)\n",
    "        out,_ = self.lstm(x)\n",
    "        last = out[:,-1,:]\n",
    "        emb = torch.relu(self.emb(last))\n",
    "        z = torch.cat([emb,x_hrv],dim=1)\n",
    "        y = self.fusion(z).squeeze(1)\n",
    "        return y\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = CNNLSTM_Fusion(hrv_dim=X.shape[1]).to(device)\n",
    "\n",
    "# ========================================================\n",
    "# 9) Train loop\n",
    "# ========================================================\n",
    "opt = torch.optim.Adam(model.parameters(),1e-3)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "for ep in range(5): # try 20+\n",
    "    model.train()\n",
    "    for sig,hrv,yb in train_loader:\n",
    "        sig,hrv,yb = sig.to(device),hrv.to(device),yb.to(device)\n",
    "        opt.zero_grad()\n",
    "        pred = model(sig,hrv)\n",
    "        loss = loss_fn(pred,yb)\n",
    "        loss.backward(); opt.step()\n",
    "    print(f\"Epoch {ep} loss {loss.item():.4f}\")\n",
    "\n",
    "# ========================================================\n",
    "# 10) Evaluate\n",
    "# ========================================================\n",
    "model.eval()\n",
    "preds,truth = [],[]\n",
    "with torch.no_grad():\n",
    "    for sig,hrv,yb in test_loader:\n",
    "        sig,hrv = sig.to(device),hrv.to(device)\n",
    "        p = model(sig,hrv)\n",
    "        preds.append(p.cpu().numpy()); truth.append(yb.numpy())\n",
    "preds = np.concatenate(preds); truth = np.concatenate(truth)\n",
    "print(\"[DL] MAE:\",mean_absolute_error(truth,preds),\"R2:\",r2_score(truth,preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64a10c47",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-09-02T21:15:11.960507Z",
     "iopub.status.busy": "2025-09-02T21:15:11.960188Z",
     "iopub.status.idle": "2025-09-02T21:15:20.740861Z",
     "shell.execute_reply": "2025-09-02T21:15:20.739642Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 8.789474,
     "end_time": "2025-09-02T21:15:20.742599",
     "exception": false,
     "start_time": "2025-09-02T21:15:11.953125",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyhrv\r\n",
      "  Downloading pyhrv-0.4.1-py3-none-any.whl.metadata (11 kB)\r\n",
      "Collecting biosppy (from pyhrv)\r\n",
      "  Downloading biosppy-2.2.3-py2.py3-none-any.whl.metadata (6.0 kB)\r\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from pyhrv) (3.7.2)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pyhrv) (1.26.4)\r\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from pyhrv) (1.15.3)\r\n",
      "Collecting nolds (from pyhrv)\r\n",
      "  Downloading nolds-0.6.2-py2.py3-none-any.whl.metadata (7.0 kB)\r\n",
      "Collecting spectrum (from pyhrv)\r\n",
      "  Downloading spectrum-0.9.0.tar.gz (231 kB)\r\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m231.5/231.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Collecting bidict (from biosppy->pyhrv)\r\n",
      "  Downloading bidict-0.23.1-py3-none-any.whl.metadata (8.7 kB)\r\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from biosppy->pyhrv) (3.14.0)\r\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from biosppy->pyhrv) (1.2.2)\r\n",
      "Collecting shortuuid (from biosppy->pyhrv)\r\n",
      "  Downloading shortuuid-1.0.13-py3-none-any.whl.metadata (5.8 kB)\r\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from biosppy->pyhrv) (1.17.0)\r\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from biosppy->pyhrv) (1.5.1)\r\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from biosppy->pyhrv) (4.11.0.86)\r\n",
      "Requirement already satisfied: pywavelets in /usr/local/lib/python3.11/dist-packages (from biosppy->pyhrv) (1.8.0)\r\n",
      "Collecting mock (from biosppy->pyhrv)\r\n",
      "  Downloading mock-5.2.0-py3-none-any.whl.metadata (3.1 kB)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pyhrv) (1.3.2)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pyhrv) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pyhrv) (4.58.4)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pyhrv) (1.4.8)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pyhrv) (25.0)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pyhrv) (11.2.1)\r\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pyhrv) (3.0.9)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pyhrv) (2.9.0.post0)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->pyhrv) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->pyhrv) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->pyhrv) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->pyhrv) (2025.2.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->pyhrv) (2022.2.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->pyhrv) (2.4.1)\r\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from nolds->pyhrv) (1.0.0)\r\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from nolds->pyhrv) (75.2.0)\r\n",
      "Collecting easydev (from spectrum->pyhrv)\r\n",
      "  Downloading easydev-0.13.3-py3-none-any.whl.metadata (4.0 kB)\r\n",
      "Requirement already satisfied: colorama<0.5.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from easydev->spectrum->pyhrv) (0.4.6)\r\n",
      "Requirement already satisfied: colorlog<7.0.0,>=6.8.2 in /usr/local/lib/python3.11/dist-packages (from easydev->spectrum->pyhrv) (6.9.0)\r\n",
      "Requirement already satisfied: line-profiler<5.0.0,>=4.1.2 in /usr/local/lib/python3.11/dist-packages (from easydev->spectrum->pyhrv) (4.2.0)\r\n",
      "Requirement already satisfied: pexpect<5.0.0,>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from easydev->spectrum->pyhrv) (4.9.0)\r\n",
      "Requirement already satisfied: platformdirs<5.0.0,>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from easydev->spectrum->pyhrv) (4.3.8)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->pyhrv) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->pyhrv) (2022.2.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->pyhrv) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->pyhrv) (2024.2.0)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->biosppy->pyhrv) (3.6.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->pyhrv) (2024.2.0)\r\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect<5.0.0,>=4.9.0->easydev->spectrum->pyhrv) (0.7.0)\r\n",
      "Downloading pyhrv-0.4.1-py3-none-any.whl (3.2 MB)\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading biosppy-2.2.3-py2.py3-none-any.whl (158 kB)\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m158.0/158.0 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nolds-0.6.2-py2.py3-none-any.whl (225 kB)\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m225.7/225.7 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading bidict-0.23.1-py3-none-any.whl (32 kB)\r\n",
      "Downloading easydev-0.13.3-py3-none-any.whl (57 kB)\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading mock-5.2.0-py3-none-any.whl (31 kB)\r\n",
      "Downloading shortuuid-1.0.13-py3-none-any.whl (10 kB)\r\n",
      "Building wheels for collected packages: spectrum\r\n",
      "  Building wheel for spectrum (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for spectrum: filename=spectrum-0.9.0-cp311-cp311-linux_x86_64.whl size=236749 sha256=e2be71fdbc48a031204efb01f0142e5ad67175cfd1569598fefcde882b59780c\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/c0/9c/de/eb558fbd03ea1540d3c908f23681f57f9d9e8c2a5cd08d6f42\r\n",
      "Successfully built spectrum\r\n",
      "Installing collected packages: shortuuid, mock, bidict, easydev, spectrum, nolds, biosppy, pyhrv\r\n",
      "Successfully installed bidict-0.23.1 biosppy-2.2.3 easydev-0.13.3 mock-5.2.0 nolds-0.6.2 pyhrv-0.4.1 shortuuid-1.0.13 spectrum-0.9.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pyhrv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fbe6529",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T21:15:20.760541Z",
     "iopub.status.busy": "2025-09-02T21:15:20.760124Z",
     "iopub.status.idle": "2025-09-02T21:16:11.940869Z",
     "shell.execute_reply": "2025-09-02T21:16:11.939817Z"
    },
    "papermill": {
     "duration": 51.191857,
     "end_time": "2025-09-02T21:16:11.942514",
     "exception": false,
     "start_time": "2025-09-02T21:15:20.750657",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading parquet: /kaggle/input/physionet-ecg/scg_rhc_f16.parquet\n",
      "Loaded parquet: /kaggle/input/physionet-ecg/scg_rhc_f16.parquet shape: (374, 20)\n",
      "Using ECG column: ECG_lead_II\n",
      "ECG sample counts per row ‚Äî min/median/max: 7500 7500 7500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e51dd3f7214461f93cd71684809b9cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Records:   0%|          | 0/374 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted feature windows: (148, 14)\n",
      "Using numeric features: ['mean_hr', 'sdnn', 'rmssd', 'pnn50', 'sd1', 'sd2', 'lf', 'hf', 'lf_hf', 'total_power']\n",
      "Windows after cleaning: 148  raw_windows: 148\n",
      "Silhouette score (k=3): 0.418\n",
      "Cluster centers:\n",
      "      mean_hr         sdnn        rmssd     pnn50          sd1          sd2  \\\n",
      "0  35.054123  1008.298631  1348.083816  0.905750   949.956762  1054.452711   \n",
      "1  28.656391  1379.848841  1733.567958  0.953869  1222.582365  1476.074905   \n",
      "2  41.217083   625.702350   854.070917  0.777838   602.627973   640.384940   \n",
      "\n",
      "              lf             hf      lf_hf   total_power  \n",
      "0  152012.988432  156448.221074   1.368202  5.030918e+05  \n",
      "1  562998.273475  107734.185549  10.616380  1.022205e+06  \n",
      "2   58508.085515   60523.649095   1.682025  1.788265e+05  \n",
      "Windows total: 148 Train windows: 118 Test windows: 30\n",
      "Starting small random search over XGBoost parameter space (24 trials)\n",
      "  tried 6 / 24 - current best MAE (cv) = 0.1141\n",
      "  tried 12 / 24 - current best MAE (cv) = 0.1095\n",
      "  tried 18 / 24 - current best MAE (cv) = 0.1066\n",
      "  tried 24 / 24 - current best MAE (cv) = 0.1066\n",
      "Random search finished. trials run: 24\n",
      "Best CV MAE on train folds: 0.10658798339380615\n",
      "Best params: {'max_depth': 4, 'learning_rate': 0.1, 'n_estimators': 800, 'subsample': 1.0, 'colsample_bytree': 0.6, 'reg_alpha': 0.001, 'reg_lambda': 1.0}\n",
      "[Final XGB] Test MAE: 0.0569 | R2: 0.9303\n",
      "Top features by XGBoost gain (partial):\n",
      "  rmssd: 0.8488\n",
      "  sdnn: 0.3152\n",
      "  total_power: 0.2111\n",
      "  sd2: 0.0582\n",
      "  lf: 0.0477\n",
      "  lf_hf: 0.0453\n",
      "  hf: 0.0321\n",
      "  mean_hr: 0.0220\n",
      "  sd1: 0.0179\n",
      "  pnn50: 0.0076\n",
      "Saved artifacts to outputs\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# Install dependencies (run once)\n",
    "# ===============================\n",
    "# (Uncomment if you need to install)\n",
    "# !pip install --quiet neurokit2 xgboost joblib scikit-learn scipy tqdm\n",
    "\n",
    "# ===============================\n",
    "# Full, robust HRV -> XGBoost pipeline\n",
    "# ===============================\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import neurokit2 as nk\n",
    "from scipy.signal import resample, welch, butter, filtfilt, iirnotch\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GroupShuffleSplit, GroupKFold\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "\n",
    "# ---------- reproducibility ----------\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# ---------- paths & params ----------\n",
    "PQ_PATH = \"/kaggle/input/physionet-ecg/scg_rhc_f16.parquet\"\n",
    "OUT_DIR = \"outputs\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "MAX_RECS = 374         # set how many records to use (use 374 to use all)\n",
    "WIN_S = 30             # window length in seconds\n",
    "HOP_S = 15             # hop in seconds\n",
    "CANDIDATE_FS = [125, 250, 360, 500]\n",
    "TARGET_FS = 250        # not strictly required for XGB, but useful if you also want fixed-length windows\n",
    "\n",
    "print(\"Loading parquet:\", PQ_PATH)\n",
    "df = pd.read_parquet(PQ_PATH)\n",
    "print(\"Loaded parquet:\", PQ_PATH, \"shape:\", df.shape)\n",
    "\n",
    "# choose ECG column\n",
    "preferred = [\"ECG_lead_II\", \"ECG_lead_ii\", \"ECG_lead_II \", \"ECG_lead_I\", \"ecg_lead_ii\"]\n",
    "ecg_col = None\n",
    "for p in preferred:\n",
    "    if p in df.columns:\n",
    "        ecg_col = p\n",
    "        break\n",
    "if ecg_col is None:\n",
    "    for c in df.columns:\n",
    "        if \"ecg\" in c.lower():\n",
    "            ecg_col = c\n",
    "            break\n",
    "if ecg_col is None:\n",
    "    raise RuntimeError(\"No ECG column found in parquet.\")\n",
    "print(\"Using ECG column:\", ecg_col)\n",
    "\n",
    "# ---------------------------\n",
    "# helper: filters & fs estimation\n",
    "# ---------------------------\n",
    "def bandpass(sig, fs, low=0.5, high=40, order=4):\n",
    "    b, a = butter(order, [low/(fs/2), high/(fs/2)], btype='band')\n",
    "    return filtfilt(b, a, sig)\n",
    "\n",
    "def notch(sig, fs, freq=50.0, q=30.0):\n",
    "    b, a = iirnotch(freq/(fs/2), q)\n",
    "    return filtfilt(b, a, sig)\n",
    "\n",
    "def estimate_fs_for_signal(arr, candidates=CANDIDATE_FS):\n",
    "    arr = np.asarray(arr, dtype=float)\n",
    "    best_fs = None\n",
    "    best_score = 1e9\n",
    "    for fs in candidates:\n",
    "        try:\n",
    "            peaks_info = nk.ecg_peaks(arr, sampling_rate=fs)[1]\n",
    "            peaks = peaks_info.get(\"ECG_R_Peaks\", [])\n",
    "            if len(peaks) < 3:\n",
    "                continue\n",
    "            rr = np.diff(peaks) / fs\n",
    "            mean_hr = 60.0 / rr.mean()\n",
    "            score = 0 if (40 <= mean_hr <= 140) else abs(mean_hr - 80)\n",
    "            if score < best_score:\n",
    "                best_score = score\n",
    "                best_fs = fs\n",
    "        except Exception:\n",
    "            continue\n",
    "    return best_fs\n",
    "\n",
    "# ---------------------------\n",
    "# robust HRV feature extraction per window\n",
    "# returns dict or None\n",
    "# ---------------------------\n",
    "def extract_hrv_features_from_window(w, fs):\n",
    "    w = np.asarray(w, dtype=float)\n",
    "    if w.size == 0 or np.max(np.abs(w)) < 1e-6:\n",
    "        return None\n",
    "    # try clean/filter\n",
    "    try:\n",
    "        wf = bandpass(w, fs)\n",
    "        wf = notch(wf, fs, freq=50.0)\n",
    "    except Exception:\n",
    "        wf = w.copy()\n",
    "    # R-peak detection\n",
    "    try:\n",
    "        signals, info = nk.ecg_process(wf, sampling_rate=fs)\n",
    "        rpeaks = np.array(info.get(\"ECG_R_Peaks\", []))\n",
    "    except Exception:\n",
    "        # fallback\n",
    "        rpeaks = np.array(nk.ecg_peaks(wf, sampling_rate=fs)[1].get(\"ECG_R_Peaks\", []))\n",
    "    if len(rpeaks) < 4:\n",
    "        return None\n",
    "    rr_s = np.diff(rpeaks) / fs            # in seconds\n",
    "    if rr_s.size < 2:\n",
    "        return None\n",
    "    rr_ms = rr_s * 1000.0                  # ms\n",
    "\n",
    "    out = {}\n",
    "    out['mean_hr'] = float(60.0 / rr_s.mean())\n",
    "    out['sdnn'] = float(np.std(rr_ms, ddof=1))\n",
    "    # RMSSD & pNN50\n",
    "    if rr_ms.size >= 2:\n",
    "        diff_rr = np.diff(rr_ms)\n",
    "        out['rmssd'] = float(np.sqrt(np.mean(diff_rr**2)))\n",
    "        out['pnn50'] = float(np.mean(np.abs(diff_rr) > 50.0))\n",
    "    else:\n",
    "        out['rmssd'] = np.nan\n",
    "        out['pnn50'] = np.nan\n",
    "    # Poincare SD1 / SD2\n",
    "    if rr_ms.size >= 2:\n",
    "        sd1 = np.sqrt(0.5) * np.std(np.diff(rr_ms))\n",
    "        sd1sq = sd1**2\n",
    "        sdnn = out['sdnn']\n",
    "        sd2sq = max(0.0, 2.0 * (sdnn**2) - sd1sq)\n",
    "        sd2 = math.sqrt(sd2sq)\n",
    "        out['sd1'] = float(sd1)\n",
    "        out['sd2'] = float(sd2)\n",
    "    else:\n",
    "        out['sd1'] = np.nan\n",
    "        out['sd2'] = np.nan\n",
    "\n",
    "    # Frequency domain via resampled tachogram -> Welch\n",
    "    try:\n",
    "        # times corresponding to each rr interval (use time at the end of each RR)\n",
    "        times = rpeaks[1:] / float(fs)\n",
    "        if len(times) >= 4:\n",
    "            fs_interp = 4.0\n",
    "            # create interp grid (ensure there is at least a few points)\n",
    "            t0, t1 = times[0], times[-1]\n",
    "            if t1 - t0 < 1.0:\n",
    "                # fallback to small uniform grid\n",
    "                t_interp = np.linspace(t0, t1, max(4, int((t1 - t0) * fs_interp)))\n",
    "            else:\n",
    "                t_interp = np.arange(t0, t1, 1.0 / fs_interp)\n",
    "                if len(t_interp) < 4:\n",
    "                    t_interp = np.linspace(t0, t1, max(4, int((t1 - t0) * fs_interp)))\n",
    "            rr_interp = np.interp(t_interp, times, rr_ms)\n",
    "            rr_interp = rr_interp - np.mean(rr_interp)\n",
    "            nperseg = min(256, len(rr_interp))\n",
    "            fxx, pxx = welch(rr_interp, fs=fs_interp, nperseg=nperseg)\n",
    "            # LF: 0.04-0.15 Hz, HF: 0.15-0.4 Hz\n",
    "            lf_mask = (fxx >= 0.04) & (fxx < 0.15)\n",
    "            hf_mask = (fxx >= 0.15) & (fxx < 0.4)\n",
    "            lf = float(np.trapz(pxx[lf_mask], fxx[lf_mask])) if lf_mask.any() else 0.0\n",
    "            hf = float(np.trapz(pxx[hf_mask], fxx[hf_mask])) if hf_mask.any() else 0.0\n",
    "            total_mask = (fxx >= 0.003) & (fxx < 0.4)\n",
    "            total_power = float(np.trapz(pxx[total_mask], fxx[total_mask])) if total_mask.any() else (lf + hf)\n",
    "            out['lf'] = lf\n",
    "            out['hf'] = hf\n",
    "            out['lf_hf'] = float(lf / hf) if (hf is not None and hf > 0) else np.nan\n",
    "            out['total_power'] = total_power\n",
    "        else:\n",
    "            out['lf'] = np.nan; out['hf'] = np.nan; out['lf_hf'] = np.nan; out['total_power'] = np.nan\n",
    "    except Exception:\n",
    "        out['lf'] = np.nan; out['hf'] = np.nan; out['lf_hf'] = np.nan; out['total_power'] = np.nan\n",
    "\n",
    "    return out\n",
    "\n",
    "# ---------------------------\n",
    "# 4) sliding windows + feature extraction\n",
    "# ---------------------------\n",
    "features = []\n",
    "raw_windows = []\n",
    "max_recs = min(MAX_RECS, len(df))\n",
    "lengths = [len(np.asarray(a, float)) for a in df[ecg_col]]\n",
    "print(\"ECG sample counts per row ‚Äî min/median/max:\",\n",
    "      int(np.min(lengths)), int(np.median(lengths)), int(np.max(lengths)))\n",
    "\n",
    "for idx in tqdm(range(max_recs), desc=\"Records\"):\n",
    "    arr = np.asarray(df[ecg_col].iloc[idx], dtype=float)\n",
    "    fs = estimate_fs_for_signal(arr)\n",
    "    if fs is None:\n",
    "        # skip unusable record\n",
    "        continue\n",
    "    npts = arr.size\n",
    "    win_pts = int(WIN_S * fs)\n",
    "    hop_pts = int(HOP_S * fs)\n",
    "    if npts < win_pts:\n",
    "        continue\n",
    "    for start in range(0, npts - win_pts + 1, hop_pts):\n",
    "        w = arr[start:start + win_pts]\n",
    "        feats = extract_hrv_features_from_window(w, fs)\n",
    "        if feats is None:\n",
    "            continue\n",
    "        rec = {\"rec_index\": int(idx), \"start_sample\": int(start), \"fs\": int(fs), \"n_samples\": int(len(w))}\n",
    "        rec.update(feats)\n",
    "        features.append(rec)\n",
    "        # store optionally the resampled window if you want DL later\n",
    "        raw_windows.append(resample(w, int(round(len(w) * (TARGET_FS / fs)))))\n",
    "\n",
    "feats_df = pd.DataFrame(features)\n",
    "print(\"Extracted feature windows:\", feats_df.shape)\n",
    "if feats_df.shape[0] == 0:\n",
    "    raise RuntimeError(\"No valid windows found; check ECG data and sampling rate detection.\")\n",
    "\n",
    "# ---------------------------\n",
    "# 5) Feature cleanup & selected feature columns\n",
    "# ---------------------------\n",
    "candidate_cols = ['mean_hr', 'sdnn', 'rmssd', 'pnn50', 'sd1', 'sd2', 'lf', 'hf', 'lf_hf', 'total_power']\n",
    "# keep only columns that exist\n",
    "candidate_cols = [c for c in candidate_cols if c in feats_df.columns]\n",
    "print(\"Using numeric features:\", candidate_cols)\n",
    "\n",
    "feats_df_clean = feats_df.dropna(subset=['mean_hr', 'sdnn']).reset_index(drop=True)  # ensure core features present\n",
    "# fill other nans with median\n",
    "for c in candidate_cols:\n",
    "    if feats_df_clean[c].isna().any():\n",
    "        feats_df_clean[c] = feats_df_clean[c].fillna(feats_df_clean[c].median())\n",
    "\n",
    "raw_windows_clean = np.array(raw_windows, dtype=object)\n",
    "# align raw_windows to feats_df_clean indices (we appended in lock-step so slicing is okay)\n",
    "if len(raw_windows_clean) != len(feats_df):\n",
    "    # if differing, try to align using valid_mask\n",
    "    valid_mask = feats_df[candidate_cols].notna().all(axis=1).values\n",
    "    raw_windows_clean = raw_windows_clean[valid_mask]\n",
    "else:\n",
    "    # apply same dropna mask used above\n",
    "    mask = feats_df.index[feats_df[['mean_hr', 'sdnn']].notna().all(axis=1)]\n",
    "    raw_windows_clean = raw_windows_clean[mask]\n",
    "\n",
    "print(\"Windows after cleaning:\", feats_df_clean.shape[0], \" raw_windows:\", raw_windows_clean.shape[0])\n",
    "\n",
    "# ---------------------------\n",
    "# 6) Unsupervised clustering -> pseudo labels (k=3) same approach as your baseline\n",
    "# ---------------------------\n",
    "X = feats_df_clean[candidate_cols].values\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "k = 3\n",
    "km = xgb.sklearn = None  # avoid name collision (we will import KMeans)\n",
    "from sklearn.cluster import KMeans\n",
    "km = KMeans(n_clusters=k, random_state=SEED, n_init=20)\n",
    "labels = km.fit_predict(X_scaled)\n",
    "sil = silhouette_score(X_scaled, labels)\n",
    "print(\"Silhouette score (k=3):\", round(sil, 3))\n",
    "\n",
    "centers_df = pd.DataFrame(scaler.inverse_transform(km.cluster_centers_), columns=candidate_cols)\n",
    "print(\"Cluster centers:\\n\", centers_df)\n",
    "\n",
    "# map cluster -> pseudo_depth by SDNN (desc -> light=0)\n",
    "order = centers_df.sort_values('sdnn', ascending=False).index.tolist()\n",
    "cluster_to_depth = {c: i for i, c in enumerate(order)}\n",
    "feats_df_clean['cluster'] = labels\n",
    "feats_df_clean['pseudo_depth'] = feats_df_clean['cluster'].map(cluster_to_depth).astype(float)\n",
    "\n",
    "# ---------------------------\n",
    "# 7) Group-aware train/test split (preserve rec_index grouping)\n",
    "# ---------------------------\n",
    "groups = feats_df_clean['rec_index'].values\n",
    "X_mat = feats_df_clean[candidate_cols].values\n",
    "y = feats_df_clean['pseudo_depth'].values\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=SEED)\n",
    "train_idx, test_idx = next(gss.split(X_mat, y, groups))\n",
    "X_train, X_test = X_mat[train_idx], X_mat[test_idx]\n",
    "y_train, y_test = y[train_idx], y[test_idx]\n",
    "groups_train = groups[train_idx]\n",
    "\n",
    "print(\"Windows total:\", len(y), \"Train windows:\", len(y_train), \"Test windows:\", len(y_test))\n",
    "\n",
    "# scale using scaler fitted above (X_scaled corresponds to whole dataset)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "# ---------------------------\n",
    "# 8) Light random search of XGBoost hyperparameters (on training folds only)\n",
    "# ---------------------------\n",
    "def cv_mean_mae_for_params(params, Xtr, ytr, groups_tr, n_splits=4):\n",
    "    # use GroupKFold on training records\n",
    "    n_groups = len(np.unique(groups_tr))\n",
    "    n_splits = min(n_splits, n_groups) if n_groups >= 2 else 2\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "    maes = []\n",
    "    for tr_idx, val_idx in gkf.split(Xtr, ytr, groups_tr):\n",
    "        model = xgb.XGBRegressor(random_state=SEED, n_jobs=1, **params)\n",
    "        model.fit(Xtr[tr_idx], ytr[tr_idx])\n",
    "        preds = model.predict(Xtr[val_idx])\n",
    "        maes.append(mean_absolute_error(ytr[val_idx], preds))\n",
    "    return np.mean(maes)\n",
    "\n",
    "# parameter search space (small, safe)\n",
    "param_choices = {\n",
    "    \"max_depth\": [3, 4, 5, 6],\n",
    "    \"learning_rate\": [0.01, 0.03, 0.05, 0.1],\n",
    "    \"n_estimators\": [100, 200, 400, 800],\n",
    "    \"subsample\": [0.7, 0.8, 0.9, 1.0],\n",
    "    \"colsample_bytree\": [0.6, 0.8, 1.0],\n",
    "    \"reg_alpha\": [0.0, 1e-3, 1e-2],\n",
    "    \"reg_lambda\": [1.0, 1.5, 2.0]\n",
    "}\n",
    "\n",
    "# sample a set of candidate configs randomly (safe count)\n",
    "n_trials = 24\n",
    "best_mae = 1e9\n",
    "best_params = None\n",
    "tried = 0\n",
    "print(\"Starting small random search over XGBoost parameter space ({} trials)\".format(n_trials))\n",
    "for _ in range(n_trials):\n",
    "    params = {\n",
    "        \"max_depth\": random.choice(param_choices[\"max_depth\"]),\n",
    "        \"learning_rate\": random.choice(param_choices[\"learning_rate\"]),\n",
    "        \"n_estimators\": random.choice(param_choices[\"n_estimators\"]),\n",
    "        \"subsample\": random.choice(param_choices[\"subsample\"]),\n",
    "        \"colsample_bytree\": random.choice(param_choices[\"colsample_bytree\"]),\n",
    "        \"reg_alpha\": random.choice(param_choices[\"reg_alpha\"]),\n",
    "        \"reg_lambda\": random.choice(param_choices[\"reg_lambda\"]),\n",
    "    }\n",
    "    try:\n",
    "        mae_cv = cv_mean_mae_for_params(params, X_train_scaled, y_train, groups_train, n_splits=4)\n",
    "    except Exception as e:\n",
    "        print(\"Trial failed:\", e)\n",
    "        continue\n",
    "    tried += 1\n",
    "    if mae_cv < best_mae:\n",
    "        best_mae = mae_cv\n",
    "        best_params = params\n",
    "    if tried % 6 == 0:\n",
    "        print(f\"  tried {tried} / {n_trials} - current best MAE (cv) = {best_mae:.4f}\")\n",
    "print(\"Random search finished. trials run:\", tried)\n",
    "print(\"Best CV MAE on train folds:\", best_mae)\n",
    "print(\"Best params:\", best_params)\n",
    "\n",
    "# if tuning failed for some reason, fall back to reasonable defaults\n",
    "if best_params is None:\n",
    "    best_params = {\"max_depth\":5, \"learning_rate\":0.05, \"n_estimators\":400, \"subsample\":0.9, \"colsample_bytree\":0.9, \"reg_alpha\":0.0, \"reg_lambda\":1.0}\n",
    "    print(\"Using fallback params:\", best_params)\n",
    "\n",
    "# ---------------------------\n",
    "# 9) Train final XGB on train set and evaluate on test set\n",
    "# ---------------------------\n",
    "final_model = xgb.XGBRegressor(random_state=SEED, n_jobs=1, **best_params)\n",
    "final_model.fit(X_train_scaled, y_train, eval_set=[(X_test_scaled, y_test)], verbose=False)\n",
    "test_preds = final_model.predict(X_test_scaled)\n",
    "test_mae = mean_absolute_error(y_test, test_preds)\n",
    "test_r2  = r2_score(y_test, test_preds)\n",
    "print(\"[Final XGB] Test MAE: {:.4f} | R2: {:.4f}\".format(test_mae, test_r2))\n",
    "\n",
    "# feature importance (by gain)\n",
    "try:\n",
    "    imp = final_model.get_booster().get_score(importance_type='gain')\n",
    "    # map back to column names\n",
    "    fi = {candidate_cols[int(k.replace('f',''))]: v for k, v in imp.items() if k.startswith('f')}\n",
    "    print(\"Top features by XGBoost gain (partial):\")\n",
    "    # sort descending\n",
    "    for k,v in sorted(fi.items(), key=lambda x: -x[1])[:10]:\n",
    "        print(f\"  {k}: {v:.4f}\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# save artifacts\n",
    "joblib.dump(final_model, os.path.join(OUT_DIR, \"xgb_final_model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(OUT_DIR, \"hrv_scaler.pkl\"))\n",
    "joblib.dump({\"candidate_cols\": candidate_cols, \"best_params\": best_params}, os.path.join(OUT_DIR, \"meta.pkl\"))\n",
    "print(\"Saved artifacts to\", OUT_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8188535,
     "sourceId": 12939936,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 73.565002,
   "end_time": "2025-09-02T21:16:12.771143",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-02T21:14:59.206141",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "04df3648c86546bd95944fa44f1c2170": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_90adda56bee4462d9f05aae75a083c9d",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_0d8748878b724d06a56b4ff2bc1b14d6",
       "tabbable": null,
       "tooltip": null,
       "value": "Records:‚Äá100%"
      }
     },
     "0c45c191942b48349912238f161863aa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "0d8748878b724d06a56b4ff2bc1b14d6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "0f6ca2f2375e48b7b1eccd804a770c10": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2e51dd3f7214461f93cd71684809b9cf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_04df3648c86546bd95944fa44f1c2170",
        "IPY_MODEL_9ce50f1efcee4c7394125ed7e6b4c096",
        "IPY_MODEL_5e9bea9086024ea98643e23fa9a1eddf"
       ],
       "layout": "IPY_MODEL_e20c9e41c3b8454db27f73036c2717b6",
       "tabbable": null,
       "tooltip": null
      }
     },
     "3ca1508d76fd45529f8683f694ce4b4e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "574911fd21fe42749ff3f97f2206ae42": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5e9bea9086024ea98643e23fa9a1eddf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_0f6ca2f2375e48b7b1eccd804a770c10",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_0c45c191942b48349912238f161863aa",
       "tabbable": null,
       "tooltip": null,
       "value": "‚Äá374/374‚Äá[00:37&lt;00:00,‚Äá10.47it/s]"
      }
     },
     "90adda56bee4462d9f05aae75a083c9d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9ce50f1efcee4c7394125ed7e6b4c096": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_574911fd21fe42749ff3f97f2206ae42",
       "max": 374.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_3ca1508d76fd45529f8683f694ce4b4e",
       "tabbable": null,
       "tooltip": null,
       "value": 374.0
      }
     },
     "e20c9e41c3b8454db27f73036c2717b6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
