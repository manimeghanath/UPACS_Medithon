{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0237269",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-02T23:31:16.724837Z",
     "iopub.status.busy": "2025-09-02T23:31:16.724483Z",
     "iopub.status.idle": "2025-09-02T23:31:18.592664Z",
     "shell.execute_reply": "2025-09-02T23:31:18.591597Z"
    },
    "papermill": {
     "duration": 1.874799,
     "end_time": "2025-09-02T23:31:18.594253",
     "exception": false,
     "start_time": "2025-09-02T23:31:16.719454",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/dreamer/DREAMER.mat\n",
      "/kaggle/input/dreamer/dreamer/DREAMER.mat\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0aac3a23",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-09-02T23:31:18.602584Z",
     "iopub.status.busy": "2025-09-02T23:31:18.601671Z",
     "iopub.status.idle": "2025-09-02T23:31:29.322272Z",
     "shell.execute_reply": "2025-09-02T23:31:29.321098Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 10.726397,
     "end_time": "2025-09-02T23:31:29.324328",
     "exception": false,
     "start_time": "2025-09-02T23:31:18.597931",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (24.1.2)\r\n",
      "Collecting pip\r\n",
      "  Downloading pip-25.2-py3-none-any.whl.metadata (4.7 kB)\r\n",
      "Downloading pip-25.2-py3-none-any.whl (1.8 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: pip\r\n",
      "  Attempting uninstall: pip\r\n",
      "    Found existing installation: pip 24.1.2\r\n",
      "    Uninstalling pip-24.1.2:\r\n",
      "      Successfully uninstalled pip-24.1.2\r\n",
      "Successfully installed pip-25.2\r\n",
      "Collecting neurokit2\r\n",
      "  Downloading neurokit2-0.2.12-py2.py3-none-any.whl.metadata (37 kB)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\r\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.5.1)\r\n",
      "Requirement already satisfied: shap in /usr/local/lib/python3.11/dist-packages (0.44.1)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from neurokit2) (2.32.4)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from neurokit2) (1.26.4)\r\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from neurokit2) (2.2.3)\r\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from neurokit2) (1.15.3)\r\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from neurokit2) (1.2.2)\r\n",
      "Requirement already satisfied: matplotlib>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from neurokit2) (3.7.2)\r\n",
      "Requirement already satisfied: PyWavelets>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from neurokit2) (1.8.0)\r\n",
      "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.11/dist-packages (from shap) (25.0)\r\n",
      "Requirement already satisfied: slicer==0.0.7 in /usr/local/lib/python3.11/dist-packages (from shap) (0.0.7)\r\n",
      "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from shap) (0.60.0)\r\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from shap) (3.1.1)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.0->neurokit2) (1.3.2)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.0->neurokit2) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.0->neurokit2) (4.58.4)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.0->neurokit2) (1.4.8)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.0->neurokit2) (11.2.1)\r\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.0->neurokit2) (3.0.9)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.0->neurokit2) (2.9.0.post0)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->neurokit2) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->neurokit2) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->neurokit2) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->neurokit2) (2025.2.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->neurokit2) (2022.2.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->neurokit2) (2.4.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.5.0->neurokit2) (1.17.0)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.0->neurokit2) (3.6.0)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->neurokit2) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->neurokit2) (2022.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->neurokit2) (2024.2.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->neurokit2) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->neurokit2) (2024.2.0)\r\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->shap) (0.43.0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->neurokit2) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->neurokit2) (2025.2)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->neurokit2) (3.4.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->neurokit2) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->neurokit2) (2.5.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->neurokit2) (2025.6.15)\r\n",
      "Downloading neurokit2-0.2.12-py2.py3-none-any.whl (708 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m708.4/708.4 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: neurokit2\r\n",
      "Successfully installed neurokit2-0.2.12\r\n"
     ]
    }
   ],
   "source": [
    "# Kaggle: run once at top of notebook\n",
    "!pip install --upgrade pip\n",
    "!pip install neurokit2 tqdm joblib shap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5843956b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T23:31:29.334260Z",
     "iopub.status.busy": "2025-09-02T23:31:29.333908Z",
     "iopub.status.idle": "2025-09-02T23:31:47.321411Z",
     "shell.execute_reply": "2025-09-02T23:31:47.320415Z"
    },
    "papermill": {
     "duration": 17.994173,
     "end_time": "2025-09-02T23:31:47.322868",
     "exception": false,
     "start_time": "2025-09-02T23:31:29.328695",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fields in DREAMER: ('Data', 'EEG_SamplingRate', 'ECG_SamplingRate', 'EEG_Electrodes', 'noOfSubjects', 'noOfVideoSequences', 'Disclaimer', 'Provider', 'Version', 'Acknowledgement')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "mat_path = '/kaggle/input/dreamer/DREAMER.mat'  # adjust if different\n",
    "mat = loadmat(mat_path)\n",
    "# The main structure is usually at mat['DREAMER'][0,0]\n",
    "dreamer = mat['DREAMER'][0,0]\n",
    "print(\"Fields in DREAMER:\", dreamer.dtype.names)\n",
    "# Often fields: Data, Labels, Subject, etc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6f2c671",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T23:31:47.332773Z",
     "iopub.status.busy": "2025-09-02T23:31:47.332289Z",
     "iopub.status.idle": "2025-09-02T23:31:58.586896Z",
     "shell.execute_reply": "2025-09-02T23:31:58.585817Z"
    },
    "papermill": {
     "duration": 11.261262,
     "end_time": "2025-09-02T23:31:58.588469",
     "exception": false,
     "start_time": "2025-09-02T23:31:47.327207",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-level keys in .mat file:\n",
      " dict_keys(['__header__', '__version__', '__globals__', 'DREAMER'])\n",
      "\n",
      "Type of DREAMER: <class 'scipy.io.matlab._mio5_params.mat_struct'>\n",
      "\n",
      "Fields in DREAMER: ['Data', 'EEG_SamplingRate', 'ECG_SamplingRate', 'EEG_Electrodes', 'noOfSubjects', 'noOfVideoSequences', 'Disclaimer', 'Provider', 'Version', 'Acknowledgement']\n",
      "\n",
      "Type of subject[0]: <class 'scipy.io.matlab._mio5_params.mat_struct'>\n",
      "Fields in subject[0]: ['Age', 'Gender', 'EEG', 'ECG', 'ScoreValence', 'ScoreArousal', 'ScoreDominance']\n",
      "\n",
      "Trial example type: <class 'NoneType'>\n"
     ]
    }
   ],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "\n",
    "# Load the DREAMER dataset\n",
    "dreamer = scipy.io.loadmat(\"/kaggle/input/dreamer/DREAMER.mat\", struct_as_record=False, squeeze_me=True)\n",
    "\n",
    "# Show top-level keys\n",
    "print(\"Top-level keys in .mat file:\\n\", dreamer.keys())\n",
    "\n",
    "# Explore Data field\n",
    "data = dreamer['DREAMER']\n",
    "print(\"\\nType of DREAMER:\", type(data))\n",
    "\n",
    "# If it has attributes, list them\n",
    "if hasattr(data, '_fieldnames'):\n",
    "    print(\"\\nFields in DREAMER:\", data._fieldnames)\n",
    "\n",
    "# Example: Check the first subject\n",
    "subject1 = data.Data[0]\n",
    "print(\"\\nType of subject[0]:\", type(subject1))\n",
    "try:\n",
    "    print(\"Fields in subject[0]:\", subject1._fieldnames)\n",
    "except:\n",
    "    print(\"Subject[0] is not a struct, type:\", type(subject1))\n",
    "\n",
    "# Peek into one trial\n",
    "trial1 = subject1[0] if isinstance(subject1, (list, np.ndarray)) else None\n",
    "print(\"\\nTrial example type:\", type(trial1))\n",
    "if trial1 is not None:\n",
    "    try:\n",
    "        print(\"Trial fields:\", trial1._fieldnames)\n",
    "    except:\n",
    "        print(\"Trial is not a struct, content:\", trial1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab38227",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T23:04:59.026284Z",
     "iopub.status.busy": "2025-09-02T23:04:59.025925Z",
     "iopub.status.idle": "2025-09-02T23:09:18.244700Z",
     "shell.execute_reply": "2025-09-02T23:09:18.243363Z",
     "shell.execute_reply.started": "2025-09-02T23:04:59.026262Z"
    },
    "papermill": {
     "duration": 0.003808,
     "end_time": "2025-09-02T23:31:58.596736",
     "exception": false,
     "start_time": "2025-09-02T23:31:58.592928",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ============================\n",
    "# UPACS — DREAMER (ECG) End-to-End (LOSO fix applied)\n",
    "# Paste into a Kaggle notebook cell and run.\n",
    "# (If NeuroKit2 not installed, uncomment pip install line)\n",
    "# ============================\n",
    "\n",
    "# !pip install --quiet neurokit2==0.2.7 joblib==1.3.2 tqdm\n",
    "\n",
    "import os, warnings, random\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "from scipy.signal import butter, filtfilt\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "import neurokit2 as nk\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.metrics import accuracy_score, f1_score, balanced_accuracy_score, roc_auc_score\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "MAT_PATH = \"/kaggle/input/dreamer/DREAMER.mat\"   # adjust if needed\n",
    "OUT_DIR = \"/kaggle/working/upacs_output\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "AROUSAL_THRESHOLD = 3.0\n",
    "MIN_RPEAKS = 5\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def bandpass_filter(sig, fs, low=0.5, high=40.0, order=4):\n",
    "    nyq = 0.5 * fs\n",
    "    lowcut, highcut = low/nyq, high/nyq\n",
    "    b, a = butter(order, [lowcut, highcut], btype=\"band\")\n",
    "    return filtfilt(b, a, sig)\n",
    "\n",
    "def find_numeric_arrays(obj, _visited=None):\n",
    "    \"\"\"Recursively find numeric numpy arrays inside nested mat_struct/object arrays.\"\"\"\n",
    "    if _visited is None:\n",
    "        _visited = set()\n",
    "    results = []\n",
    "    try:\n",
    "        oid = id(obj)\n",
    "        if oid in _visited:\n",
    "            return results\n",
    "        _visited.add(oid)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        if obj.dtype == object:\n",
    "            for el in obj.flat:\n",
    "                results.extend(find_numeric_arrays(el, _visited))\n",
    "            return results\n",
    "        if obj.dtype.names is not None:\n",
    "            for name in obj.dtype.names:\n",
    "                try:\n",
    "                    val = obj[name]\n",
    "                    results.extend(find_numeric_arrays(val, _visited))\n",
    "                except Exception:\n",
    "                    pass\n",
    "            return results\n",
    "        if np.issubdtype(obj.dtype, np.number):\n",
    "            results.append(obj)\n",
    "            return results\n",
    "        return results\n",
    "\n",
    "    # scipy mat_struct\n",
    "    if hasattr(obj, \"_fieldnames\"):\n",
    "        for name in obj._fieldnames:\n",
    "            try:\n",
    "                val = getattr(obj, name)\n",
    "                results.extend(find_numeric_arrays(val, _visited))\n",
    "            except Exception:\n",
    "                pass\n",
    "        return results\n",
    "\n",
    "    if isinstance(obj, (list, tuple)):\n",
    "        for el in obj:\n",
    "            results.extend(find_numeric_arrays(el, _visited))\n",
    "        return results\n",
    "\n",
    "    if isinstance(obj, dict):\n",
    "        for k,v in obj.items():\n",
    "            results.extend(find_numeric_arrays(v, _visited))\n",
    "        return results\n",
    "\n",
    "    try:\n",
    "        if isinstance(obj, (int, float, np.floating, np.integer)):\n",
    "            results.append(np.array([obj], dtype=float))\n",
    "            return results\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return results\n",
    "\n",
    "def choose_best_array(arr_list):\n",
    "    \"\"\"Return the longest numeric array (flattened) from list or None.\"\"\"\n",
    "    if not arr_list:\n",
    "        return None\n",
    "    arr_list = [np.asarray(a).flatten() for a in arr_list if np.asarray(a).size>0]\n",
    "    if not arr_list:\n",
    "        return None\n",
    "    arr_list_sorted = sorted(arr_list, key=lambda x: x.size, reverse=True)\n",
    "    return arr_list_sorted[0]\n",
    "\n",
    "def get_label_list(obj, expected_len=None):\n",
    "    \"\"\"Extract a python list of label values from various possible encodings.\"\"\"\n",
    "    if obj is None:\n",
    "        return None\n",
    "    try:\n",
    "        arr = np.asarray(obj, dtype=float).flatten()\n",
    "        if arr.size >= 1:\n",
    "            return arr.tolist()\n",
    "    except Exception:\n",
    "        pass\n",
    "    arrs = find_numeric_arrays(obj)\n",
    "    scalar_lists = [a.flatten() for a in arrs if np.asarray(a).ndim==1]\n",
    "    if expected_len is not None:\n",
    "        for a in scalar_lists:\n",
    "            if len(a) == expected_len:\n",
    "                return a.tolist()\n",
    "    if scalar_lists:\n",
    "        return max(scalar_lists, key=lambda x: x.size).tolist()\n",
    "    return None\n",
    "\n",
    "# ---------- Load MAT ----------\n",
    "print(\"Loading MAT:\", MAT_PATH)\n",
    "mat = loadmat(MAT_PATH, squeeze_me=True, struct_as_record=False)\n",
    "if 'DREAMER' not in mat:\n",
    "    raise RuntimeError(\"DREAMER key not found in MAT.\")\n",
    "dreamer = mat['DREAMER']\n",
    "print(\"DREAMER fields:\", dreamer._fieldnames)\n",
    "\n",
    "# sampling rate & counts\n",
    "try:\n",
    "    FS = int(dreamer.ECG_SamplingRate)\n",
    "except Exception:\n",
    "    FS = 256\n",
    "n_subjects = int(getattr(dreamer, \"noOfSubjects\", np.nan))\n",
    "n_videos = int(getattr(dreamer, \"noOfVideoSequences\", np.nan))\n",
    "print(\"FS:\", FS, \"Subjects:\", n_subjects, \"Videos:\", n_videos)\n",
    "\n",
    "# subjects list\n",
    "subjects = list(np.atleast_1d(dreamer.Data))\n",
    "print(\"Found subjects:\", len(subjects))\n",
    "\n",
    "# ---------- Extract trials robustly ----------\n",
    "rows = []\n",
    "print(\"\\nExtracting ECG trials (robustly handling mat_structs)...\")\n",
    "for subj_idx, subj in enumerate(tqdm(subjects, desc=\"Subjects\")):\n",
    "    ecg_field = getattr(subj, \"ECG\", None)\n",
    "    sv_field = getattr(subj, \"ScoreValence\", None)\n",
    "    sa_field = getattr(subj, \"ScoreArousal\", None)\n",
    "    sd_field = getattr(subj, \"ScoreDominance\", None)\n",
    "\n",
    "    # label lists\n",
    "    expected_len = None\n",
    "    try:\n",
    "        expected_len = int(n_videos) if not np.isnan(n_videos) else None\n",
    "    except:\n",
    "        expected_len = None\n",
    "    valence_list = get_label_list(sv_field, expected_len=expected_len)\n",
    "    arousal_list = get_label_list(sa_field, expected_len=expected_len)\n",
    "    dom_list = get_label_list(sd_field, expected_len=expected_len)\n",
    "\n",
    "    if ecg_field is None:\n",
    "        continue\n",
    "\n",
    "    trial_candidates = []\n",
    "    if isinstance(ecg_field, np.ndarray) and ecg_field.dtype == object:\n",
    "        for el in np.atleast_1d(ecg_field):\n",
    "            trial_candidates.append(el)\n",
    "    else:\n",
    "        arr = ecg_field\n",
    "        try:\n",
    "            np_arr = np.asarray(arr)\n",
    "            if np_arr.ndim == 2 and (not np.isnan(n_videos)):\n",
    "                r,c = np_arr.shape\n",
    "                if r == n_videos:\n",
    "                    for i in range(r):\n",
    "                        trial_candidates.append(np_arr[i,:])\n",
    "                elif c == n_videos:\n",
    "                    for i in range(c):\n",
    "                        trial_candidates.append(np_arr[:,i])\n",
    "                else:\n",
    "                    for i in range(r):\n",
    "                        trial_candidates.append(np_arr[i,:])\n",
    "            elif np_arr.ndim == 1 and (not np.isnan(n_videos)) and np_arr.size % n_videos == 0:\n",
    "                chunk = np_arr.size // n_videos\n",
    "                for i in range(n_videos):\n",
    "                    trial_candidates.append(np_arr[i*chunk:(i+1)*chunk])\n",
    "            else:\n",
    "                trial_candidates.append(arr)\n",
    "        except Exception:\n",
    "            trial_candidates.append(arr)\n",
    "\n",
    "    for t_idx, cand in enumerate(trial_candidates):\n",
    "        numeric_arrays = find_numeric_arrays(cand)\n",
    "        best = choose_best_array(numeric_arrays)\n",
    "        if best is None:\n",
    "            continue\n",
    "        if best.size < 100:\n",
    "            continue\n",
    "\n",
    "        def get_label_from_list(lst, idx):\n",
    "            try:\n",
    "                if lst is None:\n",
    "                    return np.nan\n",
    "                if idx < len(lst):\n",
    "                    return float(lst[idx])\n",
    "                else:\n",
    "                    return np.nan\n",
    "            except Exception:\n",
    "                return np.nan\n",
    "\n",
    "        val = get_label_from_list(valence_list, t_idx)\n",
    "        aro = get_label_from_list(arousal_list, t_idx)\n",
    "        dom = get_label_from_list(dom_list, t_idx)\n",
    "\n",
    "        rows.append({\n",
    "            \"subject\": subj_idx+1,\n",
    "            \"trial\": t_idx+1,\n",
    "            \"ecg\": best.astype(float).flatten(),\n",
    "            \"valence\": val,\n",
    "            \"arousal\": aro,\n",
    "            \"dominance\": dom\n",
    "        })\n",
    "\n",
    "df_trials = pd.DataFrame(rows)\n",
    "print(\"Total trials collected:\", len(df_trials))\n",
    "if len(df_trials) == 0:\n",
    "    raise RuntimeError(\"No ECG trials extracted. Inspect MAT file structure or path.\")\n",
    "\n",
    "# basic length stats\n",
    "lengths = df_trials['ecg'].apply(len)\n",
    "print(\"ECG length stats (min, median, max):\", lengths.min(), int(lengths.median()), lengths.max())\n",
    "\n",
    "# ---------- Feature extraction ----------\n",
    "def extract_features_from_ecg(ecg_signal, fs=FS, min_rpeaks=MIN_RPEAKS):\n",
    "    try:\n",
    "        x = np.asarray(ecg_signal, dtype=float)\n",
    "        if x.size < fs*2:\n",
    "            return None\n",
    "        try:\n",
    "            x_f = bandpass_filter(x, fs)\n",
    "        except Exception:\n",
    "            x_f = x\n",
    "        try:\n",
    "            signals, info = nk.ecg_process(x_f, sampling_rate=fs)\n",
    "            rpeaks = info.get(\"ECG_R_Peaks\", [])\n",
    "        except Exception:\n",
    "            try:\n",
    "                rpeaks_dict = nk.ecg_peaks(x_f, sampling_rate=fs)\n",
    "                rpeaks = rpeaks_dict.get(\"ECG_R_Peaks\", [])\n",
    "            except Exception:\n",
    "                rpeaks = []\n",
    "\n",
    "        if len(rpeaks) < min_rpeaks:\n",
    "            return None\n",
    "\n",
    "        times = np.array(rpeaks) / fs\n",
    "        rr = np.diff(times) * 1000.0\n",
    "        if len(rr) < 2:\n",
    "            return None\n",
    "\n",
    "        feats = {\n",
    "            \"mean_rr\": float(np.nanmean(rr)),\n",
    "            \"sdnn\": float(np.nanstd(rr, ddof=1)),\n",
    "            \"rmssd\": float(np.sqrt(np.nanmean(np.diff(rr)**2))),\n",
    "            \"pnn50\": float(np.sum(np.abs(np.diff(rr)) > 50.0) / max(1, (len(rr)-1)) * 100.0),\n",
    "            \"hr_mean\": float(60000.0 / np.nanmean(rr)) if np.nanmean(rr) > 0 else np.nan,\n",
    "            \"n_beats\": int(len(rpeaks))\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            hrv_t = nk.hrv_time(rpeaks, sampling_rate=fs)\n",
    "            hrv_f = nk.hrv_frequency(rpeaks, sampling_rate=fs)\n",
    "            hrv_n = nk.hrv_nonlinear(rpeaks, sampling_rate=fs)\n",
    "            for df in (hrv_t, hrv_f, hrv_n):\n",
    "                if df is None or df.shape[0] == 0:\n",
    "                    continue\n",
    "                for col in df.columns:\n",
    "                    val = df.iloc[0].get(col, np.nan)\n",
    "                    feats[str(col)] = float(val) if not pd.isna(val) else np.nan\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        return feats\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "print(\"\\nExtracting HRV features for each trial (may take a few minutes)...\")\n",
    "feature_rows = []\n",
    "for _, row in tqdm(df_trials.iterrows(), total=len(df_trials), desc=\"Trials\"):\n",
    "    feats = extract_features_from_ecg(row.ecg, fs=FS)\n",
    "    if feats is None:\n",
    "        continue\n",
    "    feats.update({\n",
    "        \"subject\": int(row.subject),\n",
    "        \"trial\": int(row.trial),\n",
    "        \"valence\": float(row.valence) if not np.isnan(row.valence) else np.nan,\n",
    "        \"arousal\": float(row.arousal) if not np.isnan(row.arousal) else np.nan,\n",
    "        \"dominance\": float(row.dominance) if not np.isnan(row.dominance) else np.nan\n",
    "    })\n",
    "    feature_rows.append(feats)\n",
    "\n",
    "features_df = pd.DataFrame(feature_rows)\n",
    "print(\"Features extracted (rows x cols):\", features_df.shape)\n",
    "if features_df.shape[0] == 0:\n",
    "    raise RuntimeError(\"No feature rows extracted — check ECG length and R-peak detection.\")\n",
    "\n",
    "features_csv = os.path.join(OUT_DIR, \"dreamer_ecg_hrv.csv\")\n",
    "features_df.to_csv(features_csv, index=False)\n",
    "print(\"Saved features CSV:\", features_csv)\n",
    "\n",
    "# drop columns with many NaNs, fill rest\n",
    "nan_frac = features_df.isna().mean()\n",
    "drop_cols = nan_frac[nan_frac > 0.4].index.tolist()\n",
    "if drop_cols:\n",
    "    print(\"Dropping columns with >40% NaN:\", drop_cols)\n",
    "    features_df.drop(columns=drop_cols, inplace=True)\n",
    "features_df.fillna(features_df.median(numeric_only=True), inplace=True)\n",
    "\n",
    "# ---------- Prepare data for modeling (Arousal binary) ----------\n",
    "features_df = features_df.dropna(subset=['arousal'])\n",
    "features_df['arousal_bin'] = (features_df['arousal'] > AROUSAL_THRESHOLD).astype(int)\n",
    "\n",
    "non_feat_cols = ['subject','trial','valence','arousal','dominance','arousal_bin']\n",
    "feature_cols = [c for c in features_df.columns if c not in non_feat_cols]\n",
    "\n",
    "X = features_df[feature_cols].values\n",
    "y = features_df['arousal_bin'].values\n",
    "groups = features_df['subject'].values\n",
    "\n",
    "print(\"Training samples:\", X.shape[0], \"Features:\", X.shape[1], \"Subjects:\", len(np.unique(groups)))\n",
    "\n",
    "scaler = StandardScaler().fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "joblib.dump(scaler, os.path.join(OUT_DIR, \"hrv_scaler.pkl\"))\n",
    "joblib.dump(feature_cols, os.path.join(OUT_DIR, \"feature_columns.pkl\"))\n",
    "\n",
    "# ---------- LOSO evaluation (ROBUST) ----------\n",
    "logo = LeaveOneGroupOut()\n",
    "accs, f1s, bals, rocs = [], [], [], []\n",
    "\n",
    "print(\"\\nRunning LOSO evaluation (robust predict_proba handling)...\")\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(logo.split(X_scaled, y, groups), start=1):\n",
    "    Xtr, Xte = X_scaled[train_idx], X_scaled[test_idx]\n",
    "    ytr, yte = y[train_idx], y[test_idx]\n",
    "\n",
    "    clf = RandomForestClassifier(n_estimators=300, class_weight='balanced', random_state=SEED, n_jobs=-1)\n",
    "    clf.fit(Xtr, ytr)\n",
    "\n",
    "    preds = clf.predict(Xte)\n",
    "\n",
    "    # Robust computation of probability for positive class (1)\n",
    "    prob = None\n",
    "    if hasattr(clf, \"predict_proba\"):\n",
    "        proba_arr = clf.predict_proba(Xte)  # shape (n_samples, n_classes_trained)\n",
    "        # If classifier trained only on one class, proba_arr has shape (n_samples,1)\n",
    "        if proba_arr.shape[1] == 1:\n",
    "            trained_class = clf.classes_[0]\n",
    "            if trained_class == 1:\n",
    "                prob = np.ones(proba_arr.shape[0])\n",
    "            else:\n",
    "                prob = np.zeros(proba_arr.shape[0])\n",
    "        else:\n",
    "            # find column corresponding to class 1 (positive)\n",
    "            if 1 in clf.classes_:\n",
    "                idx = list(clf.classes_).index(1)\n",
    "                prob = proba_arr[:, idx]\n",
    "            else:\n",
    "                # classifier has multiple classes but not 1 (unlikely here) -> zeros\n",
    "                prob = np.zeros(proba_arr.shape[0])\n",
    "    else:\n",
    "        prob = None\n",
    "\n",
    "    acc = accuracy_score(yte, preds)\n",
    "    f1 = f1_score(yte, preds, zero_division=0)\n",
    "    bal = balanced_accuracy_score(yte, preds)\n",
    "    roc = roc_auc_score(yte, prob) if (prob is not None and len(np.unique(yte))>1) else float('nan')\n",
    "\n",
    "    accs.append(acc); f1s.append(f1); bals.append(bal); rocs.append(roc)\n",
    "    subj_id = np.unique(groups[test_idx])[0] if len(np.unique(groups[test_idx]))>0 else np.nan\n",
    "    print(f\"Fold {fold_idx} (subject {subj_id}) -> Acc: {acc:.3f}, F1: {f1:.3f}, BalAcc: {bal:.3f}, ROC: {roc:.3f}\")\n",
    "\n",
    "import numpy as _np\n",
    "print(\"\\nLOSO Summary (mean ± std):\")\n",
    "print(\"Accuracy: {:.3f} ± {:.3f}\".format(_np.nanmean(accs), _np.nanstd(accs)))\n",
    "print(\"F1:       {:.3f} ± {:.3f}\".format(_np.nanmean(f1s), _np.nanstd(f1s)))\n",
    "print(\"BalAcc:   {:.3f} ± {:.3f}\".format(_np.nanmean(bals), _np.nanstd(bals)))\n",
    "print(\"ROC-AUC:  {:.3f} ± {:.3f}\".format(_np.nanmean(rocs), _np.nanstd(rocs)))\n",
    "\n",
    "# ---------- Train final model & save ----------\n",
    "final_clf = RandomForestClassifier(n_estimators=400, class_weight='balanced', random_state=SEED, n_jobs=-1)\n",
    "final_clf.fit(X_scaled, y)\n",
    "joblib.dump(final_clf, os.path.join(OUT_DIR, \"rf_arousal_model.pkl\"))\n",
    "features_df.to_csv(os.path.join(OUT_DIR, \"dreamer_ecg_hrv_cleaned.csv\"), index=False)\n",
    "print(\"\\nFinal model + cleaned CSV saved in:\", OUT_DIR)\n",
    "\n",
    "# ---------- Inference helper ----------\n",
    "def predict_from_raw_ecg(ecg_signal, model=final_clf, scaler=scaler, feature_cols=feature_cols, fs=FS):\n",
    "    feats = extract_features_from_ecg(ecg_signal, fs=fs)\n",
    "    if feats is None:\n",
    "        return {\"error\": \"Could not compute features (signal too short or no R-peaks)\", \"prob\": None, \"pred\": None}\n",
    "    x = np.array([feats.get(c, 0.0) for c in feature_cols], dtype=float).reshape(1, -1)\n",
    "    x_scaled = scaler.transform(x)\n",
    "    # robust prob extraction\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        proba = model.predict_proba(x_scaled)\n",
    "        if proba.shape[1] == 1:\n",
    "            trained_class = model.classes_[0]\n",
    "            prob_pos = float(1.0 if trained_class==1 else 0.0)\n",
    "        else:\n",
    "            if 1 in model.classes_:\n",
    "                idx = list(model.classes_).index(1)\n",
    "                prob_pos = float(proba[:, idx][0])\n",
    "            else:\n",
    "                prob_pos = 0.0\n",
    "    else:\n",
    "        prob_pos = float(model.predict(x_scaled)[0])\n",
    "    pred = int(prob_pos > 0.5)\n",
    "    return {\"prob\": prob_pos, \"pred\": pred, \"features\": feats}\n",
    "\n",
    "# quick sanity test\n",
    "if len(df_trials) > 0:\n",
    "    first_ecg = df_trials.iloc[0]['ecg']\n",
    "    print(\"\\nSanity test predict on first extracted ECG trial:\")\n",
    "    print(predict_from_raw_ecg(first_ecg))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84ca1bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T23:21:18.781574Z",
     "iopub.status.busy": "2025-09-02T23:21:18.781211Z",
     "iopub.status.idle": "2025-09-02T23:21:18.794489Z",
     "shell.execute_reply": "2025-09-02T23:21:18.793495Z",
     "shell.execute_reply.started": "2025-09-02T23:21:18.781550Z"
    },
    "papermill": {
     "duration": 0.003805,
     "end_time": "2025-09-02T23:31:58.604507",
     "exception": false,
     "start_time": "2025-09-02T23:31:58.600702",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inspect ECG fields inside subject 0\n",
    "ecg_struct = subj0.ECG\n",
    "print(\"Type of subj0.ECG:\", type(ecg_struct))\n",
    "print(\"ECG struct fields:\", ecg_struct._fieldnames)\n",
    "\n",
    "# If it has fields, check one by one\n",
    "for field in ecg_struct._fieldnames:\n",
    "    val = getattr(ecg_struct, field)\n",
    "    print(f\"\\nField: {field}\")\n",
    "    print(\"  Type:\", type(val))\n",
    "    try:\n",
    "        print(\"  Shape:\", np.shape(val))\n",
    "    except:\n",
    "        print(\"  Length:\", len(val) if hasattr(val, \"__len__\") else \"scalar\")\n",
    "    \n",
    "    # Peek first 10 values if it's array-like\n",
    "    if hasattr(val, \"__getitem__\"):\n",
    "        try:\n",
    "            print(\"  First 10 values:\", np.array(val).flatten()[:10])\n",
    "        except:\n",
    "            print(\"  Could not preview\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bab10d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T23:31:58.614212Z",
     "iopub.status.busy": "2025-09-02T23:31:58.613917Z",
     "iopub.status.idle": "2025-09-02T23:32:13.488125Z",
     "shell.execute_reply": "2025-09-02T23:32:13.486206Z"
    },
    "papermill": {
     "duration": 14.881313,
     "end_time": "2025-09-02T23:32:13.489888",
     "exception": false,
     "start_time": "2025-09-02T23:31:58.608575",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DREAMER dataset...\n",
      "Subjects: 23, Videos per subject: 18, ECG Sampling Rate: 256 Hz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting ECG trials: 100%|██████████| 23/23 [00:00<00:00, 93.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final dataset shape: (414, 6)\n",
      "   subject  trial                                                ecg  valence  \\\n",
      "0        1      1  [2046, 2056, 2042, 2063, 2039, 2059, 2039, 205...      4.0   \n",
      "1        1      2  [2054, 2061, 2036, 2041, 2036, 2041, 2035, 203...      3.0   \n",
      "2        1      3  [2018, 2026, 2022, 2026, 2025, 2024, 2027, 202...      5.0   \n",
      "3        1      4  [2055, 2051, 2052, 2051, 2053, 2054, 2054, 205...      4.0   \n",
      "4        1      5  [2080, 2080, 2038, 2052, 2043, 2062, 2044, 206...      4.0   \n",
      "\n",
      "   arousal  dominance  \n",
      "0      3.0        2.0  \n",
      "1      3.0        1.0  \n",
      "2      4.0        4.0  \n",
      "3      3.0        2.0  \n",
      "4      4.0        4.0  \n",
      "Feature matrix shape: (414, 7)\n",
      "Labels shape: (414,)\n",
      "\n",
      "Training XGBoost model...\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.31      0.38        32\n",
      "           1       0.65      0.80      0.72        51\n",
      "\n",
      "    accuracy                           0.61        83\n",
      "   macro avg       0.58      0.56      0.55        83\n",
      "weighted avg       0.59      0.61      0.59        83\n",
      "\n",
      "Accuracy: 0.6144578313253012\n",
      "\n",
      "✅ Pipeline complete: model and scaler saved.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =======================\n",
    "# 1. Load dataset\n",
    "# =======================\n",
    "print(\"Loading DREAMER dataset...\")\n",
    "mat = loadmat(\"/kaggle/input/dreamer/DREAMER.mat\", squeeze_me=True, struct_as_record=False)\n",
    "dreamer = mat[\"DREAMER\"]\n",
    "\n",
    "n_subjects = int(dreamer.noOfSubjects)\n",
    "n_videos = int(dreamer.noOfVideoSequences)\n",
    "fs = int(dreamer.ECG_SamplingRate)\n",
    "\n",
    "print(f\"Subjects: {n_subjects}, Videos per subject: {n_videos}, ECG Sampling Rate: {fs} Hz\")\n",
    "\n",
    "# =======================\n",
    "# 2. Extract ECG + Labels\n",
    "# =======================\n",
    "data = []\n",
    "\n",
    "for s in tqdm(range(n_subjects), desc=\"Extracting ECG trials\"):\n",
    "    subj = dreamer.Data[s]\n",
    "\n",
    "    ecg_baseline = subj.ECG.baseline   # shape (18,)\n",
    "    ecg_stimuli  = subj.ECG.stimuli    # shape (18,)\n",
    "\n",
    "    valence = subj.ScoreValence\n",
    "    arousal = subj.ScoreArousal\n",
    "    dominance = subj.ScoreDominance\n",
    "\n",
    "    for t in range(n_videos):\n",
    "        try:\n",
    "            ecg_trial = np.array(ecg_stimuli[t]).flatten()\n",
    "\n",
    "            # Skip if trial empty\n",
    "            if ecg_trial.size < 100:\n",
    "                continue\n",
    "\n",
    "            # Store record\n",
    "            data.append({\n",
    "                \"subject\": s+1,\n",
    "                \"trial\": t+1,\n",
    "                \"ecg\": ecg_trial,\n",
    "                \"valence\": float(valence[t]),\n",
    "                \"arousal\": float(arousal[t]),\n",
    "                \"dominance\": float(dominance[t])\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error at subject {s}, trial {t}: {e}\")\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(f\"\\nFinal dataset shape: {df.shape}\")\n",
    "print(df.head())\n",
    "\n",
    "# =======================\n",
    "# 3. Feature Extraction\n",
    "# (simple statistics per trial)\n",
    "# =======================\n",
    "def extract_features(signal):\n",
    "    return [\n",
    "        np.mean(signal),\n",
    "        np.std(signal),\n",
    "        np.min(signal),\n",
    "        np.max(signal),\n",
    "        np.median(signal),\n",
    "        np.percentile(signal, 25),\n",
    "        np.percentile(signal, 75)\n",
    "    ]\n",
    "\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    feat = extract_features(row[\"ecg\"])\n",
    "    features.append(feat)\n",
    "    # Example: classify high/low valence\n",
    "    labels.append(1 if row[\"valence\"] >= 3 else 0)\n",
    "\n",
    "X = np.array(features)\n",
    "y = np.array(labels)\n",
    "\n",
    "print(\"Feature matrix shape:\", X.shape)\n",
    "print(\"Labels shape:\", y.shape)\n",
    "\n",
    "# =======================\n",
    "# 4. Preprocessing\n",
    "# =======================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# =======================\n",
    "# 5. Train/Test Split\n",
    "# =======================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# =======================\n",
    "# 6. Train Classifier\n",
    "# =======================\n",
    "print(\"\\nTraining XGBoost model...\")\n",
    "model = XGBClassifier(n_estimators=200, max_depth=4, learning_rate=0.05, use_label_encoder=False, eval_metric=\"logloss\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# =======================\n",
    "# 7. Evaluation\n",
    "# =======================\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# =======================\n",
    "# 8. Save Model + Scaler\n",
    "# =======================\n",
    "joblib.dump(model, \"xgb_final_model.pkl\")\n",
    "joblib.dump(scaler, \"hrv_scaler.pkl\")\n",
    "\n",
    "print(\"\\n✅ Pipeline complete: model and scaler saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 208411,
     "sourceId": 455741,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 62.876534,
   "end_time": "2025-09-02T23:32:14.315298",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-02T23:31:11.438764",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
